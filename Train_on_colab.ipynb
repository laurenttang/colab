{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_on_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMY5Ygr1b6ejHKSVfuh1k5/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laurenttang/colab/blob/main/Train_on_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjDvSDU-qOuV",
        "outputId": "900748ed-59b8-4265-d53f-deb91c80934e"
      },
      "source": [
        "#!pip install keras==2.4.3\r\n",
        "#!pip install tensorflow==2.4.0\r\n",
        "!pip install keras==2.2.4\r\n",
        "!pip install tensorflow==1.13.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.2.4\n",
            "  Using cached https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.1.2)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (2.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.19.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (3.13)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed keras-2.2.4\n",
            "Collecting tensorflow==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/63/a9fa76de8dffe7455304c4ed635be4aa9c0bacef6e0633d87d5f54530c5c/tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5MB)\n",
            "\u001b[K     |████████████████████████████████| 92.5MB 43kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (3.12.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.32.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.19.4)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 58.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.36.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.10.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.3.3)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 55.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (51.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.3.3)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/03/b7e605db4a57c0f6fba744b11ef3ddf4ddebcada35022927a2b5fc623fdf/mock-4.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.4.0)\n",
            "Installing collected packages: tensorboard, mock, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.4.0\n",
            "    Uninstalling tensorboard-2.4.0:\n",
            "      Successfully uninstalled tensorboard-2.4.0\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorflow 2.4.0\n",
            "    Uninstalling tensorflow-2.4.0:\n",
            "      Successfully uninstalled tensorflow-2.4.0\n",
            "Successfully installed mock-4.0.3 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fR7ySdevTJZ",
        "outputId": "a7c9b240-9427-4294-c81b-6b6555dc7197"
      },
      "source": [
        "import keras\r\n",
        "import tensorflow \r\n",
        "print(keras.__version__)\r\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2.2.4\n",
            "1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQPne3sYIL24",
        "outputId": "576c7689-7a1a-40e8-b55a-e1560c315057"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed May 15 14:13:24 2019\n",
        "\n",
        "@author: user\n",
        "\"\"\"\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,Flatten,Conv2D,MaxPool2D,ZeroPadding2D\n",
        "from keras.layers import Activation, Dense\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential() #建立模型主體\n",
        "\n",
        "model.add(Conv2D(                        #第一層 捲積層1 輸入層\n",
        "                 filters = 32,           #建立出32個濾鏡\n",
        "                 kernel_size = (3,3),    #每個濾鏡3X3大小\n",
        "                 activation ='relu',     #定義 激活函數 為'relu'\n",
        "                 padding = 'same',       #此捲積層產生捲積影像大小不變\n",
        "                 input_shape = (29,60,3) #輸入層的資料入口格式 高29 寬60 RGB 3\n",
        "                )\n",
        "         )\n",
        "\n",
        "model.add(MaxPool2D(pool_size=(2,2)))    #第二層 池化層1 縮減取樣\n",
        "\n",
        "model.add(Dropout(0.2))                 #避免過度擬合 捨去20%的神經元\n",
        "\n",
        "model.add(Conv2D(                        #第三層 捲積層2\n",
        "                 filters = 64,           #建立出64個濾鏡\n",
        "                 kernel_size = (3,3),    #每個濾鏡3X3大小\n",
        "                 activation ='relu',     #定義 激活函數 為'relu'\n",
        "                 padding = 'same',       #此捲積層產生捲積影像大小不變\n",
        "                )\n",
        "         )\n",
        "\n",
        "model.add(MaxPool2D(pool_size=(2,2)))    #第四層 池化層2 縮減取樣\n",
        "\n",
        "model.add(Dropout(0.2))                 #避免過度擬合 捨去20%的神經元\n",
        "\n",
        "model.add(Flatten())                     #平坦層將以上特徵(3個維)轉換為1維\n",
        "\n",
        "model.add(Dropout(0.2))                 #避免過度擬合 捨去20%的神   經元\n",
        "\n",
        "model.add( Dense(                       #加入 神經網路層1 隱藏層\n",
        "                 units =10,             #此 神經網路層2 隱藏層 的神經元個數 \n",
        "                 activation ='relu'     #定義 激活函數 為'relu'\n",
        "                )\n",
        "         )\n",
        "\n",
        "model.add(Dropout(0.2))                 #避免過度擬合 捨去20%的神經元\n",
        "\n",
        "model.add(Dense(                        #加入 神經網路層2 輸出層\n",
        "                 units =4,              #此 神經網路層2 輸出層 依照答案有5個\n",
        "                 activation ='softmax'  #定義 激活函數 為'softmax'\n",
        "                )\n",
        "         )\n",
        "\n",
        "print(model.summary()) "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 29, 60, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 14, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 14, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 14, 30, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 7, 15, 64)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 7, 15, 64)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 6720)              0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 6720)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                67210     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4)                 44        \n",
            "=================================================================\n",
            "Total params: 86,646\n",
            "Trainable params: 86,646\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7Si0zFXINbP"
      },
      "source": [
        "## use colab\n",
        "#from google.colab import drive\n",
        "#import os\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "##出現提示欄進行授權\n",
        "\n",
        "#os.chdir('/content/drive/My Drive/Colab') #切換該目錄\n",
        "#os.listdir('test/') #確認目錄內容\n",
        "## =="
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s064hK71Iaa-",
        "outputId": "5a033d42-3aa0-4aa1-e61d-0a9272f362e7"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import os\n",
        "import h5py\n",
        "\n",
        "## use colab\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "##出現提示欄進行授權\n",
        "\n",
        "os.chdir('/content/drive/My Drive/Colab') #切換該目錄\n",
        "#os.listdir('test/') #確認目錄內容\n",
        "## ==\n",
        "\n",
        "ar = list(range(1, 169))              #我們創建一個1~40的空間\n",
        "\n",
        "x_image_train=[]                     #裝圖片的變數\n",
        "y_label_train = []                   #裝答案的變數\n",
        "\n",
        "Long = os.listdir(\"TrainSample/Resizedata/Long\")\n",
        "Short = os.listdir(\"TrainSample/Resizedata/Short\")\n",
        "Swing = os.listdir(\"TrainSample/Resizedata/Swing\")\n",
        "\n",
        "for index in Long :                    #跑回圈 ar =1~40\n",
        "    img =  cv2.imread(\"TrainSample/Resizedata/Long/\"+index)          #使用CV2讀取圖片\n",
        "    x_image_train.append(img)        #將圖片加入裝圖片的變數\n",
        "    y_label_train.append([1])\n",
        "for index in Short :                    #跑回圈 ar =1~40\n",
        "    img =  cv2.imread(\"TrainSample/Resizedata/Short/\"+index)          #使用CV2讀取圖片\n",
        "    x_image_train.append(img)        #將圖片加入裝圖片的變數\n",
        "    y_label_train.append([2])\n",
        "for index in Swing :                    #跑回圈 ar =1~40\n",
        "    img =  cv2.imread(\"TrainSample/Resizedata/Swing/\"+index)          #使用CV2讀取圖片\n",
        "    x_image_train.append(img)        #將圖片加入裝圖片的變數\n",
        "    y_label_train.append([3])                                  \n",
        "\n",
        "        \n",
        "x_image_train=np.asarray(x_image_train)    #轉成array\n",
        "y_label_train =np.asarray(y_label_train)\n",
        "#for i in range(0,len(x_image_train)):\n",
        "#    print(type(x_image_train[i]))\n",
        "print(x_image_train.shape)                #查看arrayru結構，40張圖片+相片高29像素+相片寬60像素+RGB 3原色\n",
        "\n",
        "x_image_train = x_image_train.astype('float32') / 255.0  #將數字影像標準化\n",
        "\n",
        "print(x_image_train)\n",
        "\n",
        "from keras.utils import np_utils\n",
        "y_label_train = np_utils.to_categorical(y_label_train) #答案以Onehot encoding 轉換\n",
        "print(y_label_train)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "(504, 29, 60, 3)\n",
            "[[[[1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   ...\n",
            "   [1.         0.99607843 1.        ]\n",
            "   [1.         0.99607843 1.        ]\n",
            "   [0.9764706  0.972549   0.98039216]]\n",
            "\n",
            "  [[1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   ...\n",
            "   [1.         0.99607843 1.        ]\n",
            "   [1.         0.99607843 1.        ]\n",
            "   [1.         0.99607843 1.        ]]\n",
            "\n",
            "  [[1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   ...\n",
            "   [0.9372549  0.93333334 0.9411765 ]\n",
            "   [0.9764706  0.972549   0.98039216]\n",
            "   [1.         0.99607843 1.        ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.9764706  0.99215686 0.972549  ]\n",
            "   [0.9764706  0.99215686 0.972549  ]\n",
            "   [0.5294118  0.54509807 0.5254902 ]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  [[0.99215686 1.         0.99607843]\n",
            "   [0.99215686 1.         0.9882353 ]\n",
            "   [0.47843137 0.49019608 0.48235294]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  [[0.98039216 0.9882353  0.9882353 ]\n",
            "   [0.99215686 1.         0.99607843]\n",
            "   [0.9764706  0.9843137  0.9843137 ]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]]\n",
            "\n",
            "\n",
            " [[[1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   ...\n",
            "   [0.8862745  0.9607843  0.89411765]\n",
            "   [0.9647059  1.         0.972549  ]\n",
            "   [0.95686275 1.         0.9647059 ]]\n",
            "\n",
            "  [[1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   ...\n",
            "   [0.73333335 0.8        0.74509805]\n",
            "   [0.89411765 0.9490196  0.9019608 ]\n",
            "   [0.9647059  1.         0.972549  ]]\n",
            "\n",
            "  [[1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   ...\n",
            "   [0.7529412  0.81960785 0.7647059 ]\n",
            "   [0.8901961  0.94509804 0.8980392 ]\n",
            "   [0.972549   1.         0.9764706 ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.9843137  0.99607843 1.        ]\n",
            "   [0.9843137  0.99607843 1.        ]\n",
            "   [0.9647059  0.96862745 1.        ]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  [[0.9647059  0.972549   0.972549  ]\n",
            "   [0.9490196  0.9647059  0.96862745]\n",
            "   [0.99215686 0.99215686 1.        ]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  [[1.         1.         0.9882353 ]\n",
            "   [0.99215686 1.         0.99607843]\n",
            "   [1.         0.99607843 1.        ]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]]\n",
            "\n",
            "\n",
            " [[[1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   ...\n",
            "   [1.         1.         0.99607843]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  [[1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   ...\n",
            "   [1.         1.         0.99607843]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  [[1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.98039216 1.         0.9843137 ]\n",
            "   [0.92156863 0.9529412  0.9254902 ]\n",
            "   [0.57254905 0.59607846 0.5764706 ]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  [[0.9647059  0.99607843 0.96862745]\n",
            "   [0.98039216 1.         0.9843137 ]\n",
            "   [0.9529412  0.9764706  0.95686275]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  [[0.92941177 0.9607843  0.93333334]\n",
            "   [0.98039216 1.         0.9843137 ]\n",
            "   [0.98039216 1.         0.9843137 ]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  [[1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  [[1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.99215686 1.         0.9490196 ]\n",
            "   [0.96862745 1.         0.93333334]\n",
            "   [0.47058824 0.49803922 0.4392157 ]\n",
            "   ...\n",
            "   [1.         1.         0.9882353 ]\n",
            "   [1.         0.99607843 1.        ]\n",
            "   [1.         0.99215686 1.        ]]\n",
            "\n",
            "  [[1.         1.         0.9607843 ]\n",
            "   [0.9647059  0.99215686 0.93333334]\n",
            "   [0.6039216  0.61960787 0.57254905]\n",
            "   ...\n",
            "   [1.         0.99607843 1.        ]\n",
            "   [1.         0.99607843 1.        ]\n",
            "   [1.         0.99607843 1.        ]]\n",
            "\n",
            "  [[1.         1.         0.972549  ]\n",
            "   [1.         1.         0.9764706 ]\n",
            "   [0.9529412  0.95686275 0.92156863]\n",
            "   ...\n",
            "   [1.         0.99607843 1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]]\n",
            "\n",
            "\n",
            " [[[1.         1.         0.99607843]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         0.99215686 1.        ]\n",
            "   ...\n",
            "   [1.         0.9882353  1.        ]\n",
            "   [1.         0.9882353  1.        ]\n",
            "   [0.98039216 0.9647059  0.9882353 ]]\n",
            "\n",
            "  [[1.         1.         0.99607843]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         0.99215686 1.        ]\n",
            "   ...\n",
            "   [0.99215686 0.9647059  1.        ]\n",
            "   [0.99215686 0.96862745 1.        ]\n",
            "   [1.         0.9882353  1.        ]]\n",
            "\n",
            "  [[1.         1.         0.99607843]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         0.99607843 1.        ]\n",
            "   ...\n",
            "   [1.         0.9764706  1.        ]\n",
            "   [1.         0.98039216 1.        ]\n",
            "   [1.         0.98039216 1.        ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  [[1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  [[1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]]\n",
            "\n",
            "\n",
            " [[[0.99215686 0.9882353  1.        ]\n",
            "   [0.98039216 0.9764706  1.        ]\n",
            "   [0.96862745 0.9647059  1.        ]\n",
            "   ...\n",
            "   [1.         1.         0.99607843]\n",
            "   [1.         1.         0.99607843]\n",
            "   [1.         1.         0.99607843]]\n",
            "\n",
            "  [[0.9098039  0.90588236 0.9411765 ]\n",
            "   [0.99215686 0.9882353  1.        ]\n",
            "   [0.972549   0.96862745 1.        ]\n",
            "   ...\n",
            "   [1.         1.         0.99607843]\n",
            "   [1.         1.         0.99607843]\n",
            "   [1.         1.         0.99607843]]\n",
            "\n",
            "  [[0.99215686 0.9882353  1.        ]\n",
            "   [0.9372549  0.93333334 0.96862745]\n",
            "   [0.6039216  0.6        0.63529414]\n",
            "   ...\n",
            "   [1.         1.         0.99607843]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[1.         0.99607843 1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   ...\n",
            "   [1.         1.         0.9882353 ]\n",
            "   [1.         1.         0.9882353 ]\n",
            "   [1.         1.         0.9882353 ]]\n",
            "\n",
            "  [[1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   ...\n",
            "   [1.         1.         0.9843137 ]\n",
            "   [1.         1.         0.9882353 ]\n",
            "   [1.         1.         0.9882353 ]]\n",
            "\n",
            "  [[1.         1.         1.        ]\n",
            "   [0.99215686 1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   ...\n",
            "   [1.         1.         0.9843137 ]\n",
            "   [1.         1.         0.9882353 ]\n",
            "   [1.         1.         0.9882353 ]]]]\n",
            "[[0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MrEpMdYnIaxK",
        "outputId": "272f310e-b930-4a1b-86f4-b30f0b14ab96"
      },
      "source": [
        "##定義模型訓練方式\n",
        "from keras import metrics\n",
        "model.compile(                                 #設定模型\n",
        "              loss='mean_squared_error',       #損失函數 此例使用 均方誤差\n",
        "              optimizer = 'adam',              #優化方法 此例使用 adam\n",
        "              metrics =['accuracy']            #評估模型的方式 此例我們希望的是準確率()\n",
        "             )\n",
        "model.optimizer.lr.assign(0.05)\n",
        "\n",
        "train_model = model.fit(x = x_image_train,             #資料裡對應的特徵\n",
        "                        y = y_label_train,             #資料裡對應的結果\n",
        "                        epochs = 50000,                   #訓練週期 此例設定為20\n",
        "                        batch_size = 512,                #訓練批次筆數 此例設定為2\n",
        "                        verbose = 2                    #顯示訓練過程\n",
        "                       )\n",
        "\n",
        "\n",
        "\n",
        "#保存模型\n",
        "model.save('Recognize_Ver4.h5')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/50000\n",
            " - 2s - loss: 0.1873 - acc: 0.2956\n",
            "Epoch 2/50000\n",
            " - 2s - loss: 0.1744 - acc: 0.3135\n",
            "Epoch 3/50000\n",
            " - 2s - loss: 0.1750 - acc: 0.3373\n",
            "Epoch 4/50000\n",
            " - 2s - loss: 0.1758 - acc: 0.3492\n",
            "Epoch 5/50000\n",
            " - 2s - loss: 0.1726 - acc: 0.3651\n",
            "Epoch 6/50000\n",
            " - 2s - loss: 0.1742 - acc: 0.3333\n",
            "Epoch 7/50000\n",
            " - 2s - loss: 0.1746 - acc: 0.3135\n",
            "Epoch 8/50000\n",
            " - 2s - loss: 0.1694 - acc: 0.3571\n",
            "Epoch 9/50000\n",
            " - 2s - loss: 0.1719 - acc: 0.3214\n",
            "Epoch 10/50000\n",
            " - 2s - loss: 0.1691 - acc: 0.3552\n",
            "Epoch 11/50000\n",
            " - 2s - loss: 0.1703 - acc: 0.3433\n",
            "Epoch 12/50000\n",
            " - 2s - loss: 0.1691 - acc: 0.4107\n",
            "Epoch 13/50000\n",
            " - 2s - loss: 0.1665 - acc: 0.4762\n",
            "Epoch 14/50000\n",
            " - 2s - loss: 0.1663 - acc: 0.4385\n",
            "Epoch 15/50000\n",
            " - 2s - loss: 0.1649 - acc: 0.4742\n",
            "Epoch 16/50000\n",
            " - 2s - loss: 0.1625 - acc: 0.4762\n",
            "Epoch 17/50000\n",
            " - 2s - loss: 0.1596 - acc: 0.5476\n",
            "Epoch 18/50000\n",
            " - 2s - loss: 0.1585 - acc: 0.5635\n",
            "Epoch 19/50000\n",
            " - 2s - loss: 0.1542 - acc: 0.6270\n",
            "Epoch 20/50000\n",
            " - 2s - loss: 0.1481 - acc: 0.6627\n",
            "Epoch 21/50000\n",
            " - 2s - loss: 0.1427 - acc: 0.6766\n",
            "Epoch 22/50000\n",
            " - 2s - loss: 0.1359 - acc: 0.7242\n",
            "Epoch 23/50000\n",
            " - 2s - loss: 0.1305 - acc: 0.7341\n",
            "Epoch 24/50000\n",
            " - 2s - loss: 0.1265 - acc: 0.7202\n",
            "Epoch 25/50000\n",
            " - 2s - loss: 0.1242 - acc: 0.7341\n",
            "Epoch 26/50000\n",
            " - 2s - loss: 0.1128 - acc: 0.7738\n",
            "Epoch 27/50000\n",
            " - 2s - loss: 0.1065 - acc: 0.7540\n",
            "Epoch 28/50000\n",
            " - 2s - loss: 0.1048 - acc: 0.7123\n",
            "Epoch 29/50000\n",
            " - 2s - loss: 0.0977 - acc: 0.7718\n",
            "Epoch 30/50000\n",
            " - 2s - loss: 0.0935 - acc: 0.7698\n",
            "Epoch 31/50000\n",
            " - 2s - loss: 0.0895 - acc: 0.7599\n",
            "Epoch 32/50000\n",
            " - 2s - loss: 0.0855 - acc: 0.7877\n",
            "Epoch 33/50000\n",
            " - 2s - loss: 0.0873 - acc: 0.7698\n",
            "Epoch 34/50000\n",
            " - 2s - loss: 0.0805 - acc: 0.7758\n",
            "Epoch 35/50000\n",
            " - 2s - loss: 0.0758 - acc: 0.7857\n",
            "Epoch 36/50000\n",
            " - 2s - loss: 0.0793 - acc: 0.7877\n",
            "Epoch 37/50000\n",
            " - 2s - loss: 0.0684 - acc: 0.8095\n",
            "Epoch 38/50000\n",
            " - 2s - loss: 0.0676 - acc: 0.8075\n",
            "Epoch 39/50000\n",
            " - 2s - loss: 0.0645 - acc: 0.8175\n",
            "Epoch 40/50000\n",
            " - 2s - loss: 0.0760 - acc: 0.7758\n",
            "Epoch 41/50000\n",
            " - 2s - loss: 0.0650 - acc: 0.8095\n",
            "Epoch 42/50000\n",
            " - 2s - loss: 0.0653 - acc: 0.8075\n",
            "Epoch 43/50000\n",
            " - 2s - loss: 0.0591 - acc: 0.8333\n",
            "Epoch 44/50000\n",
            " - 2s - loss: 0.0648 - acc: 0.8095\n",
            "Epoch 45/50000\n",
            " - 2s - loss: 0.0611 - acc: 0.7976\n",
            "Epoch 46/50000\n",
            " - 2s - loss: 0.0548 - acc: 0.8313\n",
            "Epoch 47/50000\n",
            " - 2s - loss: 0.0617 - acc: 0.8155\n",
            "Epoch 48/50000\n",
            " - 2s - loss: 0.0627 - acc: 0.8254\n",
            "Epoch 49/50000\n",
            " - 2s - loss: 0.0587 - acc: 0.8472\n",
            "Epoch 50/50000\n",
            " - 2s - loss: 0.0561 - acc: 0.8373\n",
            "Epoch 51/50000\n",
            " - 2s - loss: 0.0604 - acc: 0.8472\n",
            "Epoch 52/50000\n",
            " - 2s - loss: 0.0556 - acc: 0.8512\n",
            "Epoch 53/50000\n",
            " - 2s - loss: 0.0591 - acc: 0.8492\n",
            "Epoch 54/50000\n",
            " - 2s - loss: 0.0567 - acc: 0.8413\n",
            "Epoch 55/50000\n",
            " - 2s - loss: 0.0448 - acc: 0.8968\n",
            "Epoch 56/50000\n",
            " - 2s - loss: 0.0503 - acc: 0.8690\n",
            "Epoch 57/50000\n",
            " - 2s - loss: 0.0528 - acc: 0.8591\n",
            "Epoch 58/50000\n",
            " - 2s - loss: 0.0518 - acc: 0.8690\n",
            "Epoch 59/50000\n",
            " - 2s - loss: 0.0478 - acc: 0.8770\n",
            "Epoch 60/50000\n",
            " - 2s - loss: 0.0508 - acc: 0.8690\n",
            "Epoch 61/50000\n",
            " - 2s - loss: 0.0454 - acc: 0.8849\n",
            "Epoch 62/50000\n",
            " - 2s - loss: 0.0484 - acc: 0.8671\n",
            "Epoch 63/50000\n",
            " - 2s - loss: 0.0507 - acc: 0.8750\n",
            "Epoch 64/50000\n",
            " - 2s - loss: 0.0452 - acc: 0.8591\n",
            "Epoch 65/50000\n",
            " - 2s - loss: 0.0434 - acc: 0.8889\n",
            "Epoch 66/50000\n",
            " - 2s - loss: 0.0447 - acc: 0.8790\n",
            "Epoch 67/50000\n",
            " - 2s - loss: 0.0469 - acc: 0.8770\n",
            "Epoch 68/50000\n",
            " - 2s - loss: 0.0445 - acc: 0.8730\n",
            "Epoch 69/50000\n",
            " - 2s - loss: 0.0431 - acc: 0.9028\n",
            "Epoch 70/50000\n",
            " - 2s - loss: 0.0451 - acc: 0.8710\n",
            "Epoch 71/50000\n",
            " - 2s - loss: 0.0475 - acc: 0.8770\n",
            "Epoch 72/50000\n",
            " - 2s - loss: 0.0440 - acc: 0.8869\n",
            "Epoch 73/50000\n",
            " - 2s - loss: 0.0486 - acc: 0.8869\n",
            "Epoch 74/50000\n",
            " - 2s - loss: 0.0447 - acc: 0.8889\n",
            "Epoch 75/50000\n",
            " - 2s - loss: 0.0381 - acc: 0.9107\n",
            "Epoch 76/50000\n",
            " - 2s - loss: 0.0447 - acc: 0.8690\n",
            "Epoch 77/50000\n",
            " - 2s - loss: 0.0474 - acc: 0.8829\n",
            "Epoch 78/50000\n",
            " - 2s - loss: 0.0435 - acc: 0.8730\n",
            "Epoch 79/50000\n",
            " - 2s - loss: 0.0541 - acc: 0.8413\n",
            "Epoch 80/50000\n",
            " - 2s - loss: 0.0385 - acc: 0.8909\n",
            "Epoch 81/50000\n",
            " - 2s - loss: 0.0446 - acc: 0.8829\n",
            "Epoch 82/50000\n",
            " - 2s - loss: 0.0443 - acc: 0.8849\n",
            "Epoch 83/50000\n",
            " - 2s - loss: 0.0397 - acc: 0.8810\n",
            "Epoch 84/50000\n",
            " - 2s - loss: 0.0367 - acc: 0.8988\n",
            "Epoch 85/50000\n",
            " - 2s - loss: 0.0379 - acc: 0.9087\n",
            "Epoch 86/50000\n",
            " - 2s - loss: 0.0424 - acc: 0.9048\n",
            "Epoch 87/50000\n",
            " - 2s - loss: 0.0458 - acc: 0.8810\n",
            "Epoch 88/50000\n",
            " - 2s - loss: 0.0423 - acc: 0.8790\n",
            "Epoch 89/50000\n",
            " - 2s - loss: 0.0431 - acc: 0.8968\n",
            "Epoch 90/50000\n",
            " - 2s - loss: 0.0379 - acc: 0.9028\n",
            "Epoch 91/50000\n",
            " - 2s - loss: 0.0392 - acc: 0.9028\n",
            "Epoch 92/50000\n",
            " - 2s - loss: 0.0424 - acc: 0.8869\n",
            "Epoch 93/50000\n",
            " - 2s - loss: 0.0419 - acc: 0.8770\n",
            "Epoch 94/50000\n",
            " - 2s - loss: 0.0433 - acc: 0.8829\n",
            "Epoch 95/50000\n",
            " - 2s - loss: 0.0330 - acc: 0.9127\n",
            "Epoch 96/50000\n",
            " - 2s - loss: 0.0374 - acc: 0.8968\n",
            "Epoch 97/50000\n",
            " - 2s - loss: 0.0383 - acc: 0.8909\n",
            "Epoch 98/50000\n",
            " - 2s - loss: 0.0370 - acc: 0.9087\n",
            "Epoch 99/50000\n",
            " - 2s - loss: 0.0334 - acc: 0.9028\n",
            "Epoch 100/50000\n",
            " - 2s - loss: 0.0391 - acc: 0.8929\n",
            "Epoch 101/50000\n",
            " - 2s - loss: 0.0407 - acc: 0.8889\n",
            "Epoch 102/50000\n",
            " - 2s - loss: 0.0374 - acc: 0.8988\n",
            "Epoch 103/50000\n",
            " - 2s - loss: 0.0337 - acc: 0.9206\n",
            "Epoch 104/50000\n",
            " - 2s - loss: 0.0351 - acc: 0.8988\n",
            "Epoch 105/50000\n",
            " - 2s - loss: 0.0424 - acc: 0.8810\n",
            "Epoch 106/50000\n",
            " - 2s - loss: 0.0350 - acc: 0.8988\n",
            "Epoch 107/50000\n",
            " - 2s - loss: 0.0356 - acc: 0.9028\n",
            "Epoch 108/50000\n",
            " - 2s - loss: 0.0360 - acc: 0.8849\n",
            "Epoch 109/50000\n",
            " - 2s - loss: 0.0421 - acc: 0.8770\n",
            "Epoch 110/50000\n",
            " - 2s - loss: 0.0392 - acc: 0.9028\n",
            "Epoch 111/50000\n",
            " - 2s - loss: 0.0392 - acc: 0.8889\n",
            "Epoch 112/50000\n",
            " - 2s - loss: 0.0391 - acc: 0.8849\n",
            "Epoch 113/50000\n",
            " - 2s - loss: 0.0412 - acc: 0.8869\n",
            "Epoch 114/50000\n",
            " - 2s - loss: 0.0390 - acc: 0.8810\n",
            "Epoch 115/50000\n",
            " - 2s - loss: 0.0352 - acc: 0.9028\n",
            "Epoch 116/50000\n",
            " - 2s - loss: 0.0398 - acc: 0.8988\n",
            "Epoch 117/50000\n",
            " - 2s - loss: 0.0352 - acc: 0.8988\n",
            "Epoch 118/50000\n",
            " - 2s - loss: 0.0399 - acc: 0.8909\n",
            "Epoch 119/50000\n",
            " - 2s - loss: 0.0332 - acc: 0.9187\n",
            "Epoch 120/50000\n",
            " - 2s - loss: 0.0343 - acc: 0.9087\n",
            "Epoch 121/50000\n",
            " - 2s - loss: 0.0332 - acc: 0.9147\n",
            "Epoch 122/50000\n",
            " - 2s - loss: 0.0338 - acc: 0.9087\n",
            "Epoch 123/50000\n",
            " - 2s - loss: 0.0291 - acc: 0.9187\n",
            "Epoch 124/50000\n",
            " - 2s - loss: 0.0407 - acc: 0.8849\n",
            "Epoch 125/50000\n",
            " - 2s - loss: 0.0402 - acc: 0.8948\n",
            "Epoch 126/50000\n",
            " - 2s - loss: 0.0361 - acc: 0.9067\n",
            "Epoch 127/50000\n",
            " - 2s - loss: 0.0396 - acc: 0.8750\n",
            "Epoch 128/50000\n",
            " - 2s - loss: 0.0348 - acc: 0.8968\n",
            "Epoch 129/50000\n",
            " - 2s - loss: 0.0366 - acc: 0.8948\n",
            "Epoch 130/50000\n",
            " - 2s - loss: 0.0332 - acc: 0.9107\n",
            "Epoch 131/50000\n",
            " - 2s - loss: 0.0346 - acc: 0.9048\n",
            "Epoch 132/50000\n",
            " - 2s - loss: 0.0343 - acc: 0.8988\n",
            "Epoch 133/50000\n",
            " - 2s - loss: 0.0342 - acc: 0.9167\n",
            "Epoch 134/50000\n",
            " - 2s - loss: 0.0339 - acc: 0.9008\n",
            "Epoch 135/50000\n",
            " - 2s - loss: 0.0395 - acc: 0.9008\n",
            "Epoch 136/50000\n",
            " - 2s - loss: 0.0339 - acc: 0.9087\n",
            "Epoch 137/50000\n",
            " - 2s - loss: 0.0373 - acc: 0.8948\n",
            "Epoch 138/50000\n",
            " - 2s - loss: 0.0318 - acc: 0.9226\n",
            "Epoch 139/50000\n",
            " - 2s - loss: 0.0398 - acc: 0.8770\n",
            "Epoch 140/50000\n",
            " - 2s - loss: 0.0370 - acc: 0.8790\n",
            "Epoch 141/50000\n",
            " - 2s - loss: 0.0422 - acc: 0.8829\n",
            "Epoch 142/50000\n",
            " - 2s - loss: 0.0413 - acc: 0.8849\n",
            "Epoch 143/50000\n",
            " - 2s - loss: 0.0437 - acc: 0.8631\n",
            "Epoch 144/50000\n",
            " - 2s - loss: 0.0324 - acc: 0.9226\n",
            "Epoch 145/50000\n",
            " - 2s - loss: 0.0420 - acc: 0.8750\n",
            "Epoch 146/50000\n",
            " - 2s - loss: 0.0312 - acc: 0.9187\n",
            "Epoch 147/50000\n",
            " - 2s - loss: 0.0342 - acc: 0.9067\n",
            "Epoch 148/50000\n",
            " - 2s - loss: 0.0359 - acc: 0.9067\n",
            "Epoch 149/50000\n",
            " - 2s - loss: 0.0416 - acc: 0.8671\n",
            "Epoch 150/50000\n",
            " - 2s - loss: 0.0363 - acc: 0.8909\n",
            "Epoch 151/50000\n",
            " - 2s - loss: 0.0408 - acc: 0.8849\n",
            "Epoch 152/50000\n",
            " - 2s - loss: 0.0408 - acc: 0.8869\n",
            "Epoch 153/50000\n",
            " - 2s - loss: 0.0334 - acc: 0.9087\n",
            "Epoch 154/50000\n",
            " - 2s - loss: 0.0371 - acc: 0.8909\n",
            "Epoch 155/50000\n",
            " - 2s - loss: 0.0373 - acc: 0.8889\n",
            "Epoch 156/50000\n",
            " - 2s - loss: 0.0368 - acc: 0.9067\n",
            "Epoch 157/50000\n",
            " - 2s - loss: 0.0389 - acc: 0.8770\n",
            "Epoch 158/50000\n",
            " - 2s - loss: 0.0324 - acc: 0.9147\n",
            "Epoch 159/50000\n",
            " - 2s - loss: 0.0402 - acc: 0.8849\n",
            "Epoch 160/50000\n",
            " - 2s - loss: 0.0354 - acc: 0.8909\n",
            "Epoch 161/50000\n",
            " - 2s - loss: 0.0324 - acc: 0.9028\n",
            "Epoch 162/50000\n",
            " - 2s - loss: 0.0362 - acc: 0.9067\n",
            "Epoch 163/50000\n",
            " - 2s - loss: 0.0361 - acc: 0.8790\n",
            "Epoch 164/50000\n",
            " - 2s - loss: 0.0329 - acc: 0.9067\n",
            "Epoch 165/50000\n",
            " - 2s - loss: 0.0370 - acc: 0.8948\n",
            "Epoch 166/50000\n",
            " - 2s - loss: 0.0337 - acc: 0.9206\n",
            "Epoch 167/50000\n",
            " - 2s - loss: 0.0360 - acc: 0.8968\n",
            "Epoch 168/50000\n",
            " - 2s - loss: 0.0348 - acc: 0.8968\n",
            "Epoch 169/50000\n",
            " - 2s - loss: 0.0329 - acc: 0.9127\n",
            "Epoch 170/50000\n",
            " - 2s - loss: 0.0360 - acc: 0.8929\n",
            "Epoch 171/50000\n",
            " - 2s - loss: 0.0321 - acc: 0.9127\n",
            "Epoch 172/50000\n",
            " - 2s - loss: 0.0329 - acc: 0.9187\n",
            "Epoch 173/50000\n",
            " - 2s - loss: 0.0374 - acc: 0.8889\n",
            "Epoch 174/50000\n",
            " - 2s - loss: 0.0285 - acc: 0.9226\n",
            "Epoch 175/50000\n",
            " - 2s - loss: 0.0327 - acc: 0.9187\n",
            "Epoch 176/50000\n",
            " - 2s - loss: 0.0380 - acc: 0.8929\n",
            "Epoch 177/50000\n",
            " - 2s - loss: 0.0324 - acc: 0.9147\n",
            "Epoch 178/50000\n",
            " - 2s - loss: 0.0337 - acc: 0.9048\n",
            "Epoch 179/50000\n",
            " - 2s - loss: 0.0342 - acc: 0.8988\n",
            "Epoch 180/50000\n",
            " - 2s - loss: 0.0342 - acc: 0.8929\n",
            "Epoch 181/50000\n",
            " - 2s - loss: 0.0307 - acc: 0.9167\n",
            "Epoch 182/50000\n",
            " - 2s - loss: 0.0290 - acc: 0.9187\n",
            "Epoch 183/50000\n",
            " - 2s - loss: 0.0299 - acc: 0.9048\n",
            "Epoch 184/50000\n",
            " - 2s - loss: 0.0334 - acc: 0.9067\n",
            "Epoch 185/50000\n",
            " - 2s - loss: 0.0313 - acc: 0.9028\n",
            "Epoch 186/50000\n",
            " - 2s - loss: 0.0332 - acc: 0.8929\n",
            "Epoch 187/50000\n",
            " - 2s - loss: 0.0313 - acc: 0.9048\n",
            "Epoch 188/50000\n",
            " - 2s - loss: 0.0341 - acc: 0.9087\n",
            "Epoch 189/50000\n",
            " - 2s - loss: 0.0311 - acc: 0.9127\n",
            "Epoch 190/50000\n",
            " - 2s - loss: 0.0294 - acc: 0.9187\n",
            "Epoch 191/50000\n",
            " - 2s - loss: 0.0308 - acc: 0.9067\n",
            "Epoch 192/50000\n",
            " - 2s - loss: 0.0341 - acc: 0.8909\n",
            "Epoch 193/50000\n",
            " - 2s - loss: 0.0285 - acc: 0.9266\n",
            "Epoch 194/50000\n",
            " - 2s - loss: 0.0386 - acc: 0.8909\n",
            "Epoch 195/50000\n",
            " - 2s - loss: 0.0276 - acc: 0.9206\n",
            "Epoch 196/50000\n",
            " - 2s - loss: 0.0323 - acc: 0.8929\n",
            "Epoch 197/50000\n",
            " - 2s - loss: 0.0328 - acc: 0.9008\n",
            "Epoch 198/50000\n",
            " - 2s - loss: 0.0340 - acc: 0.8869\n",
            "Epoch 199/50000\n",
            " - 2s - loss: 0.0294 - acc: 0.9147\n",
            "Epoch 200/50000\n",
            " - 2s - loss: 0.0276 - acc: 0.9206\n",
            "Epoch 201/50000\n",
            " - 2s - loss: 0.0268 - acc: 0.9266\n",
            "Epoch 202/50000\n",
            " - 2s - loss: 0.0325 - acc: 0.8968\n",
            "Epoch 203/50000\n",
            " - 2s - loss: 0.0305 - acc: 0.9206\n",
            "Epoch 204/50000\n",
            " - 2s - loss: 0.0292 - acc: 0.9107\n",
            "Epoch 205/50000\n",
            " - 2s - loss: 0.0332 - acc: 0.8909\n",
            "Epoch 206/50000\n",
            " - 2s - loss: 0.0333 - acc: 0.8968\n",
            "Epoch 207/50000\n",
            " - 2s - loss: 0.0329 - acc: 0.8948\n",
            "Epoch 208/50000\n",
            " - 2s - loss: 0.0304 - acc: 0.9008\n",
            "Epoch 209/50000\n",
            " - 2s - loss: 0.0326 - acc: 0.8869\n",
            "Epoch 210/50000\n",
            " - 2s - loss: 0.0333 - acc: 0.8968\n",
            "Epoch 211/50000\n",
            " - 2s - loss: 0.0323 - acc: 0.9028\n",
            "Epoch 212/50000\n",
            " - 2s - loss: 0.0288 - acc: 0.9206\n",
            "Epoch 213/50000\n",
            " - 2s - loss: 0.0350 - acc: 0.8889\n",
            "Epoch 214/50000\n",
            " - 2s - loss: 0.0363 - acc: 0.8968\n",
            "Epoch 215/50000\n",
            " - 2s - loss: 0.0279 - acc: 0.9187\n",
            "Epoch 216/50000\n",
            " - 2s - loss: 0.0329 - acc: 0.9107\n",
            "Epoch 217/50000\n",
            " - 2s - loss: 0.0278 - acc: 0.9067\n",
            "Epoch 218/50000\n",
            " - 2s - loss: 0.0333 - acc: 0.9008\n",
            "Epoch 219/50000\n",
            " - 2s - loss: 0.0308 - acc: 0.9127\n",
            "Epoch 220/50000\n",
            " - 2s - loss: 0.0286 - acc: 0.9087\n",
            "Epoch 221/50000\n",
            " - 2s - loss: 0.0319 - acc: 0.9107\n",
            "Epoch 222/50000\n",
            " - 2s - loss: 0.0288 - acc: 0.9167\n",
            "Epoch 223/50000\n",
            " - 2s - loss: 0.0357 - acc: 0.8988\n",
            "Epoch 224/50000\n",
            " - 2s - loss: 0.0352 - acc: 0.8968\n",
            "Epoch 225/50000\n",
            " - 2s - loss: 0.0275 - acc: 0.9067\n",
            "Epoch 226/50000\n",
            " - 2s - loss: 0.0306 - acc: 0.9107\n",
            "Epoch 227/50000\n",
            " - 2s - loss: 0.0343 - acc: 0.8929\n",
            "Epoch 228/50000\n",
            " - 2s - loss: 0.0312 - acc: 0.8988\n",
            "Epoch 229/50000\n",
            " - 2s - loss: 0.0308 - acc: 0.9067\n",
            "Epoch 230/50000\n",
            " - 2s - loss: 0.0321 - acc: 0.8869\n",
            "Epoch 231/50000\n",
            " - 2s - loss: 0.0314 - acc: 0.9087\n",
            "Epoch 232/50000\n",
            " - 2s - loss: 0.0300 - acc: 0.9147\n",
            "Epoch 233/50000\n",
            " - 2s - loss: 0.0341 - acc: 0.9087\n",
            "Epoch 234/50000\n",
            " - 2s - loss: 0.0295 - acc: 0.9147\n",
            "Epoch 235/50000\n",
            " - 2s - loss: 0.0296 - acc: 0.9266\n",
            "Epoch 236/50000\n",
            " - 2s - loss: 0.0289 - acc: 0.9067\n",
            "Epoch 237/50000\n",
            " - 2s - loss: 0.0293 - acc: 0.9147\n",
            "Epoch 238/50000\n",
            " - 2s - loss: 0.0257 - acc: 0.9345\n",
            "Epoch 239/50000\n",
            " - 2s - loss: 0.0324 - acc: 0.9147\n",
            "Epoch 240/50000\n",
            " - 2s - loss: 0.0289 - acc: 0.9087\n",
            "Epoch 241/50000\n",
            " - 2s - loss: 0.0264 - acc: 0.9246\n",
            "Epoch 242/50000\n",
            " - 2s - loss: 0.0251 - acc: 0.9325\n",
            "Epoch 243/50000\n",
            " - 2s - loss: 0.0268 - acc: 0.9385\n",
            "Epoch 244/50000\n",
            " - 2s - loss: 0.0323 - acc: 0.8829\n",
            "Epoch 245/50000\n",
            " - 2s - loss: 0.0314 - acc: 0.9107\n",
            "Epoch 246/50000\n",
            " - 2s - loss: 0.0331 - acc: 0.9028\n",
            "Epoch 247/50000\n",
            " - 2s - loss: 0.0299 - acc: 0.9127\n",
            "Epoch 248/50000\n",
            " - 2s - loss: 0.0249 - acc: 0.9306\n",
            "Epoch 249/50000\n",
            " - 2s - loss: 0.0321 - acc: 0.9206\n",
            "Epoch 250/50000\n",
            " - 2s - loss: 0.0288 - acc: 0.9246\n",
            "Epoch 251/50000\n",
            " - 2s - loss: 0.0302 - acc: 0.9107\n",
            "Epoch 252/50000\n",
            " - 2s - loss: 0.0290 - acc: 0.9266\n",
            "Epoch 253/50000\n",
            " - 2s - loss: 0.0297 - acc: 0.9087\n",
            "Epoch 254/50000\n",
            " - 2s - loss: 0.0314 - acc: 0.9028\n",
            "Epoch 255/50000\n",
            " - 2s - loss: 0.0294 - acc: 0.9147\n",
            "Epoch 256/50000\n",
            " - 2s - loss: 0.0301 - acc: 0.9028\n",
            "Epoch 257/50000\n",
            " - 2s - loss: 0.0290 - acc: 0.9306\n",
            "Epoch 258/50000\n",
            " - 2s - loss: 0.0299 - acc: 0.8889\n",
            "Epoch 259/50000\n",
            " - 2s - loss: 0.0329 - acc: 0.8829\n",
            "Epoch 260/50000\n",
            " - 2s - loss: 0.0274 - acc: 0.9206\n",
            "Epoch 261/50000\n",
            " - 2s - loss: 0.0321 - acc: 0.9206\n",
            "Epoch 262/50000\n",
            " - 2s - loss: 0.0321 - acc: 0.9107\n",
            "Epoch 263/50000\n",
            " - 2s - loss: 0.0292 - acc: 0.9107\n",
            "Epoch 264/50000\n",
            " - 2s - loss: 0.0300 - acc: 0.9107\n",
            "Epoch 265/50000\n",
            " - 2s - loss: 0.0249 - acc: 0.9246\n",
            "Epoch 266/50000\n",
            " - 2s - loss: 0.0323 - acc: 0.9107\n",
            "Epoch 267/50000\n",
            " - 2s - loss: 0.0308 - acc: 0.9107\n",
            "Epoch 268/50000\n",
            " - 2s - loss: 0.0351 - acc: 0.8968\n",
            "Epoch 269/50000\n",
            " - 2s - loss: 0.0289 - acc: 0.9266\n",
            "Epoch 270/50000\n",
            " - 2s - loss: 0.0334 - acc: 0.8909\n",
            "Epoch 271/50000\n",
            " - 2s - loss: 0.0321 - acc: 0.9048\n",
            "Epoch 272/50000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-5bb13374f39f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                         \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m                   \u001b[0;31m#訓練週期 此例設定為20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m                \u001b[0;31m#訓練批次筆數 此例設定為2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                         \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m                    \u001b[0;31m#顯示訓練過程\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                        )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Oy0diUkU8Ia"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('Recognize_Ver4.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}