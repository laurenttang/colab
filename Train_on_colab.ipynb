{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_on_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNOPArtPhXlU2C/mzG1hDoY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laurenttang/colab/blob/main/Train_on_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaGFkMQc4722",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06ea613a-3fc0-47c7-c2ae-b3eb29465cda"
      },
      "source": [
        "gpu_info = !nvidia-smi\r\n",
        "gpu_info = '\\n'.join(gpu_info)\r\n",
        "if gpu_info.find('failed') >= 0:\r\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\r\n",
        "  print('and then re-execute this cell.')\r\n",
        "else:\r\n",
        "  print(gpu_info)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Dec 28 11:07:04 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.27.04    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjDvSDU-qOuV",
        "outputId": "44bf1022-1275-4c8d-bb92-d0f7ff3d1bd3"
      },
      "source": [
        "#!pip install keras==2.4.3\r\n",
        "#!pip install tensorflow==2.4.0\r\n",
        "!pip install keras==2.2.4\r\n",
        "!pip install tensorflow==1.13.1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras==2.2.4 in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.19.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.1.2)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: tensorflow==1.13.1 in /usr/local/lib/python3.6/dist-packages (1.13.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.10.0)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.13.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (3.12.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.32.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.36.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.19.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.3.3)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (51.0.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (4.0.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fR7ySdevTJZ",
        "outputId": "c8ebf6bc-7bde-479c-a9e8-3cd301fd8ffd"
      },
      "source": [
        "import keras\r\n",
        "import tensorflow \r\n",
        "print(keras.__version__)\r\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2.2.4\n",
            "1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQPne3sYIL24",
        "outputId": "0c22cd6c-cece-442d-c0fc-02619e359f27"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed May 15 14:13:24 2019\n",
        "\n",
        "@author: user\n",
        "\"\"\"\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,Flatten,Conv2D,MaxPool2D,ZeroPadding2D\n",
        "from keras.layers import Activation, Dense\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential() #建立模型主體\n",
        "\n",
        "model.add(Conv2D(                        #第一層 捲積層1 輸入層\n",
        "                 filters = 32,           #建立出32個濾鏡\n",
        "                 kernel_size = (3,3),    #每個濾鏡3X3大小\n",
        "                 activation ='relu',     #定義 激活函數 為'relu'\n",
        "                 padding = 'same',       #此捲積層產生捲積影像大小不變\n",
        "                 input_shape = (29,60,3) #輸入層的資料入口格式 高29 寬60 RGB 3\n",
        "                )\n",
        "         )\n",
        "\n",
        "model.add(MaxPool2D(pool_size=(2,2)))    #第二層 池化層1 縮減取樣\n",
        "\n",
        "model.add(Dropout(0.2))                 #避免過度擬合 捨去20%的神經元\n",
        "\n",
        "model.add(Conv2D(                        #第三層 捲積層2\n",
        "                 filters = 64,           #建立出64個濾鏡\n",
        "                 kernel_size = (3,3),    #每個濾鏡3X3大小\n",
        "                 activation ='relu',     #定義 激活函數 為'relu'\n",
        "                 padding = 'same',       #此捲積層產生捲積影像大小不變\n",
        "                )\n",
        "         )\n",
        "\n",
        "model.add(MaxPool2D(pool_size=(2,2)))    #第四層 池化層2 縮減取樣\n",
        "\n",
        "model.add(Dropout(0.2))                 #避免過度擬合 捨去20%的神經元\n",
        "\n",
        "model.add(Flatten())                     #平坦層將以上特徵(3個維)轉換為1維\n",
        "\n",
        "model.add(Dropout(0.2))                 #避免過度擬合 捨去20%的神   經元\n",
        "\n",
        "model.add( Dense(                       #加入 神經網路層1 隱藏層\n",
        "                 units =10,             #此 神經網路層2 隱藏層 的神經元個數 \n",
        "                 activation ='relu'     #定義 激活函數 為'relu'\n",
        "                )\n",
        "         )\n",
        "\n",
        "model.add(Dropout(0.2))                 #避免過度擬合 捨去20%的神經元\n",
        "\n",
        "model.add(Dense(                        #加入 神經網路層2 輸出層\n",
        "                 units =5,              #此 神經網路層2 輸出層 依照答案有5個\n",
        "                 activation ='softmax'  #定義 激活函數 為'softmax'\n",
        "                )\n",
        "         )\n",
        "\n",
        "print(model.summary()) "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 29, 60, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 14, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 14, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 14, 30, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 7, 15, 64)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 7, 15, 64)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 6720)              0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 6720)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                67210     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 55        \n",
            "=================================================================\n",
            "Total params: 86,657\n",
            "Trainable params: 86,657\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7Si0zFXINbP"
      },
      "source": [
        "## use colab\n",
        "#from google.colab import drive\n",
        "#import os\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "##出現提示欄進行授權\n",
        "\n",
        "#os.chdir('/content/drive/My Drive/Colab') #切換該目錄\n",
        "#os.listdir('test/') #確認目錄內容\n",
        "## =="
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s064hK71Iaa-",
        "outputId": "a58f06d9-427d-46a9-b397-9901348311f4"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import os\n",
        "import h5py\n",
        "\n",
        "## use colab\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "##出現提示欄進行授權\n",
        "\n",
        "os.chdir('/content/drive/My Drive/Colab') #切換該目錄\n",
        "#os.listdir('test/') #確認目錄內容\n",
        "## ==\n",
        "\n",
        "ar = list(range(1, 169))              #我們創建一個1~40的空間\n",
        "\n",
        "x_image_train=[]                     #裝圖片的變數\n",
        "y_label_train = []                   #裝答案的變數\n",
        "\n",
        "Long = os.listdir(\"TrainSample/Resizedata/Long\")\n",
        "Short = os.listdir(\"TrainSample/Resizedata/Short\")\n",
        "Swing = os.listdir(\"TrainSample/Resizedata/Swing\")\n",
        "WBottom = os.listdir(\"TrainSample/Resizedata/WBottom\")\n",
        "\n",
        "for index in Long :                    #跑回圈 ar =1~40\n",
        "    img =  cv2.imread(\"TrainSample/Resizedata/Long/\"+index)          #使用CV2讀取圖片\n",
        "    x_image_train.append(img)        #將圖片加入裝圖片的變數\n",
        "    y_label_train.append([1])\n",
        "for index in Short :                    #跑回圈 ar =1~40\n",
        "    img =  cv2.imread(\"TrainSample/Resizedata/Short/\"+index)          #使用CV2讀取圖片\n",
        "    x_image_train.append(img)        #將圖片加入裝圖片的變數\n",
        "    y_label_train.append([2])\n",
        "for index in Swing :                    #跑回圈 ar =1~40\n",
        "    img =  cv2.imread(\"TrainSample/Resizedata/Swing/\"+index)          #使用CV2讀取圖片\n",
        "    x_image_train.append(img)        #將圖片加入裝圖片的變數\n",
        "    y_label_train.append([3])                                  \n",
        "for index in WBottom :                    #跑回圈 ar =1~40\n",
        "    img =  cv2.imread(\"TrainSample/Resizedata/WBottom/\"+index)          #使用CV2讀取圖片\n",
        "    x_image_train.append(img)        #將圖片加入裝圖片的變數\n",
        "    y_label_train.append([4])\n",
        "        \n",
        "x_image_train=np.asarray(x_image_train)    #轉成array\n",
        "y_label_train =np.asarray(y_label_train)\n",
        "#for i in range(0,len(x_image_train)):\n",
        "#    print(type(x_image_train[i]))\n",
        "print(x_image_train.shape)                #查看arrayru結構，40張圖片+相片高29像素+相片寬60像素+RGB 3原色\n",
        "\n",
        "x_image_train = x_image_train.astype('float32') / 255.0  #將數字影像標準化\n",
        "\n",
        "print(x_image_train)\n",
        "\n",
        "from keras.utils import np_utils\n",
        "y_label_train = np_utils.to_categorical(y_label_train) #答案以Onehot encoding 轉換\n",
        "print(y_label_train)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "(672, 29, 60, 3)\n",
            "[[[[1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   ...\n",
            "   [1.         0.99607843 1.        ]\n",
            "   [1.         0.99607843 1.        ]\n",
            "   [0.9764706  0.972549   0.98039216]]\n",
            "\n",
            "  [[1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   ...\n",
            "   [1.         0.99607843 1.        ]\n",
            "   [1.         0.99607843 1.        ]\n",
            "   [1.         0.99607843 1.        ]]\n",
            "\n",
            "  [[1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   ...\n",
            "   [0.9372549  0.93333334 0.9411765 ]\n",
            "   [0.9764706  0.972549   0.98039216]\n",
            "   [1.         0.99607843 1.        ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.9764706  0.99215686 0.972549  ]\n",
            "   [0.9764706  0.99215686 0.972549  ]\n",
            "   [0.5294118  0.54509807 0.5254902 ]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  [[0.99215686 1.         0.99607843]\n",
            "   [0.99215686 1.         0.9882353 ]\n",
            "   [0.47843137 0.49019608 0.48235294]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  [[0.98039216 0.9882353  0.9882353 ]\n",
            "   [0.99215686 1.         0.99607843]\n",
            "   [0.9764706  0.9843137  0.9843137 ]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]]\n",
            "\n",
            "\n",
            " [[[1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   ...\n",
            "   [0.8862745  0.9607843  0.89411765]\n",
            "   [0.9647059  1.         0.972549  ]\n",
            "   [0.95686275 1.         0.9647059 ]]\n",
            "\n",
            "  [[1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   ...\n",
            "   [0.73333335 0.8        0.74509805]\n",
            "   [0.89411765 0.9490196  0.9019608 ]\n",
            "   [0.9647059  1.         0.972549  ]]\n",
            "\n",
            "  [[1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   ...\n",
            "   [0.7529412  0.81960785 0.7647059 ]\n",
            "   [0.8901961  0.94509804 0.8980392 ]\n",
            "   [0.972549   1.         0.9764706 ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.9843137  0.99607843 1.        ]\n",
            "   [0.9843137  0.99607843 1.        ]\n",
            "   [0.9647059  0.96862745 1.        ]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  [[0.9647059  0.972549   0.972549  ]\n",
            "   [0.9490196  0.9647059  0.96862745]\n",
            "   [0.99215686 0.99215686 1.        ]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  [[1.         1.         0.9882353 ]\n",
            "   [0.99215686 1.         0.99607843]\n",
            "   [1.         0.99607843 1.        ]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]]\n",
            "\n",
            "\n",
            " [[[1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   ...\n",
            "   [1.         1.         0.99607843]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  [[1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   ...\n",
            "   [1.         1.         0.99607843]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  [[1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.98039216 1.         0.9843137 ]\n",
            "   [0.92156863 0.9529412  0.9254902 ]\n",
            "   [0.57254905 0.59607846 0.5764706 ]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  [[0.9647059  0.99607843 0.96862745]\n",
            "   [0.98039216 1.         0.9843137 ]\n",
            "   [0.9529412  0.9764706  0.95686275]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  [[0.92941177 0.9607843  0.93333334]\n",
            "   [0.98039216 1.         0.9843137 ]\n",
            "   [0.98039216 1.         0.9843137 ]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[0.972549   0.98039216 0.98039216]\n",
            "   [0.99215686 1.         1.        ]\n",
            "   [0.99215686 1.         1.        ]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  [[0.98039216 0.9882353  0.9882353 ]\n",
            "   [0.99215686 1.         1.        ]\n",
            "   [0.98039216 0.9882353  0.9882353 ]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  [[0.99215686 1.         1.        ]\n",
            "   [0.99215686 1.         1.        ]\n",
            "   [0.9843137  0.99215686 0.99215686]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  [[0.99215686 0.99215686 0.99215686]\n",
            "   [0.99215686 0.99215686 0.99215686]\n",
            "   [0.99215686 0.99215686 0.99215686]\n",
            "   ...\n",
            "   [0.99215686 0.99215686 0.99215686]\n",
            "   [0.99215686 0.99215686 0.99215686]\n",
            "   [0.99215686 0.99215686 0.99215686]]\n",
            "\n",
            "  [[0.94509804 0.94509804 0.94509804]\n",
            "   [0.94509804 0.94509804 0.94509804]\n",
            "   [0.94509804 0.94509804 0.94509804]\n",
            "   ...\n",
            "   [0.94509804 0.94509804 0.94509804]\n",
            "   [0.94509804 0.94509804 0.94509804]\n",
            "   [0.94509804 0.94509804 0.94509804]]]\n",
            "\n",
            "\n",
            " [[[0.98039216 0.99607843 1.        ]\n",
            "   [0.98039216 0.99607843 1.        ]\n",
            "   [0.9372549  0.9529412  0.972549  ]\n",
            "   ...\n",
            "   [0.9843137  0.99607843 1.        ]\n",
            "   [1.         0.99607843 1.        ]\n",
            "   [1.         0.99607843 1.        ]]\n",
            "\n",
            "  [[0.972549   0.9882353  1.        ]\n",
            "   [0.98039216 0.99607843 1.        ]\n",
            "   [0.9607843  0.9764706  0.99607843]\n",
            "   ...\n",
            "   [0.9843137  0.99607843 1.        ]\n",
            "   [0.99215686 1.         1.        ]\n",
            "   [1.         0.99607843 1.        ]]\n",
            "\n",
            "  [[0.98039216 0.99215686 1.        ]\n",
            "   [0.9843137  0.99607843 1.        ]\n",
            "   [0.9411765  0.9529412  0.96862745]\n",
            "   ...\n",
            "   [0.9843137  0.99607843 1.        ]\n",
            "   [0.99215686 1.         1.        ]\n",
            "   [0.99215686 1.         1.        ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  [[0.99215686 0.99215686 0.99215686]\n",
            "   [0.99215686 0.99215686 0.99215686]\n",
            "   [0.99215686 0.99215686 0.99215686]\n",
            "   ...\n",
            "   [0.99215686 0.99215686 0.99215686]\n",
            "   [0.99215686 0.99215686 0.99215686]\n",
            "   [0.99215686 0.99215686 0.99215686]]\n",
            "\n",
            "  [[0.94509804 0.94509804 0.94509804]\n",
            "   [0.94509804 0.94509804 0.94509804]\n",
            "   [0.94509804 0.94509804 0.94509804]\n",
            "   ...\n",
            "   [0.94509804 0.94509804 0.94509804]\n",
            "   [0.94509804 0.94509804 0.94509804]\n",
            "   [0.94509804 0.94509804 0.94509804]]]\n",
            "\n",
            "\n",
            " [[[0.9843137  1.         0.98039216]\n",
            "   [0.9843137  1.         0.98039216]\n",
            "   [0.9411765  0.9529412  0.94509804]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  [[0.9843137  1.         0.98039216]\n",
            "   [0.99215686 1.         0.9882353 ]\n",
            "   [0.8666667  0.8784314  0.87058824]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  [[0.9647059  0.98039216 0.9607843 ]\n",
            "   [0.99215686 1.         0.9882353 ]\n",
            "   [0.90588236 0.91764706 0.9098039 ]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   ...\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  [[0.99215686 0.99215686 0.99215686]\n",
            "   [0.99215686 0.99215686 0.99215686]\n",
            "   [0.99215686 0.99215686 0.99215686]\n",
            "   ...\n",
            "   [0.99215686 0.99215686 0.99215686]\n",
            "   [0.99215686 0.99215686 0.99215686]\n",
            "   [0.99215686 0.99607843 0.9882353 ]]\n",
            "\n",
            "  [[0.94509804 0.94509804 0.94509804]\n",
            "   [0.94509804 0.94509804 0.94509804]\n",
            "   [0.94509804 0.94509804 0.94509804]\n",
            "   ...\n",
            "   [0.94509804 0.9490196  0.9411765 ]\n",
            "   [0.94509804 0.9490196  0.9411765 ]\n",
            "   [0.94509804 0.9490196  0.93333334]]]]\n",
            "[[0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MrEpMdYnIaxK",
        "outputId": "64f32c84-efc6-4182-8d36-d1955ccdff36"
      },
      "source": [
        "##定義模型訓練方式\n",
        "from keras import metrics\n",
        "model.compile(                                 #設定模型\n",
        "              loss='mean_squared_error',       #損失函數 此例使用 均方誤差\n",
        "              optimizer = 'adam',              #優化方法 此例使用 adam\n",
        "              metrics =['accuracy']            #評估模型的方式 此例我們希望的是準確率()\n",
        "             )\n",
        "model.optimizer.lr.assign(0.05)\n",
        "\n",
        "train_model = model.fit(x = x_image_train,             #資料裡對應的特徵\n",
        "                        y = y_label_train,             #資料裡對應的結果\n",
        "                        epochs = 50000,                   #訓練週期 此例設定為20\n",
        "                        batch_size = 512,                #訓練批次筆數 此例設定為2\n",
        "                        verbose = 2                    #顯示訓練過程\n",
        "                       )\n",
        "\n",
        "\n",
        "\n",
        "#保存模型\n",
        "model.save('Recognize_Ver4.h5')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/50000\n",
            " - 3s - loss: 0.1617 - acc: 0.2202\n",
            "Epoch 2/50000\n",
            " - 2s - loss: 0.1557 - acc: 0.2634\n",
            "Epoch 3/50000\n",
            " - 2s - loss: 0.1555 - acc: 0.2649\n",
            "Epoch 4/50000\n",
            " - 2s - loss: 0.1551 - acc: 0.2470\n",
            "Epoch 5/50000\n",
            " - 2s - loss: 0.1541 - acc: 0.2619\n",
            "Epoch 6/50000\n",
            " - 2s - loss: 0.1544 - acc: 0.2411\n",
            "Epoch 7/50000\n",
            " - 2s - loss: 0.1529 - acc: 0.2783\n",
            "Epoch 8/50000\n",
            " - 2s - loss: 0.1518 - acc: 0.2946\n",
            "Epoch 9/50000\n",
            " - 2s - loss: 0.1504 - acc: 0.3571\n",
            "Epoch 10/50000\n",
            " - 2s - loss: 0.1516 - acc: 0.3006\n",
            "Epoch 11/50000\n",
            " - 2s - loss: 0.1481 - acc: 0.3735\n",
            "Epoch 12/50000\n",
            " - 2s - loss: 0.1481 - acc: 0.3810\n",
            "Epoch 13/50000\n",
            " - 2s - loss: 0.1452 - acc: 0.4494\n",
            "Epoch 14/50000\n",
            " - 2s - loss: 0.1419 - acc: 0.4167\n",
            "Epoch 15/50000\n",
            " - 2s - loss: 0.1374 - acc: 0.5030\n",
            "Epoch 16/50000\n",
            " - 2s - loss: 0.1339 - acc: 0.5298\n",
            "Epoch 17/50000\n",
            " - 2s - loss: 0.1302 - acc: 0.5164\n",
            "Epoch 18/50000\n",
            " - 2s - loss: 0.1254 - acc: 0.5595\n",
            "Epoch 19/50000\n",
            " - 2s - loss: 0.1196 - acc: 0.6027\n",
            "Epoch 20/50000\n",
            " - 2s - loss: 0.1124 - acc: 0.6458\n",
            "Epoch 21/50000\n",
            " - 2s - loss: 0.1059 - acc: 0.6637\n",
            "Epoch 22/50000\n",
            " - 2s - loss: 0.1026 - acc: 0.6488\n",
            "Epoch 23/50000\n",
            " - 2s - loss: 0.0934 - acc: 0.6905\n",
            "Epoch 24/50000\n",
            " - 2s - loss: 0.0901 - acc: 0.6756\n",
            "Epoch 25/50000\n",
            " - 2s - loss: 0.0821 - acc: 0.7321\n",
            "Epoch 26/50000\n",
            " - 2s - loss: 0.0783 - acc: 0.7351\n",
            "Epoch 27/50000\n",
            " - 2s - loss: 0.0744 - acc: 0.7530\n",
            "Epoch 28/50000\n",
            " - 2s - loss: 0.0662 - acc: 0.7440\n",
            "Epoch 29/50000\n",
            " - 2s - loss: 0.0792 - acc: 0.7054\n",
            "Epoch 30/50000\n",
            " - 2s - loss: 0.0743 - acc: 0.7113\n",
            "Epoch 31/50000\n",
            " - 2s - loss: 0.0653 - acc: 0.7530\n",
            "Epoch 32/50000\n",
            " - 2s - loss: 0.0614 - acc: 0.7589\n",
            "Epoch 33/50000\n",
            " - 2s - loss: 0.0677 - acc: 0.7232\n",
            "Epoch 34/50000\n",
            " - 2s - loss: 0.0589 - acc: 0.7723\n",
            "Epoch 35/50000\n",
            " - 2s - loss: 0.0613 - acc: 0.7515\n",
            "Epoch 36/50000\n",
            " - 2s - loss: 0.0542 - acc: 0.7917\n",
            "Epoch 37/50000\n",
            " - 2s - loss: 0.0537 - acc: 0.7857\n",
            "Epoch 38/50000\n",
            " - 2s - loss: 0.0560 - acc: 0.7798\n",
            "Epoch 39/50000\n",
            " - 2s - loss: 0.0573 - acc: 0.7679\n",
            "Epoch 40/50000\n",
            " - 2s - loss: 0.0522 - acc: 0.7798\n",
            "Epoch 41/50000\n",
            " - 2s - loss: 0.0598 - acc: 0.7470\n",
            "Epoch 42/50000\n",
            " - 2s - loss: 0.0535 - acc: 0.7798\n",
            "Epoch 43/50000\n",
            " - 2s - loss: 0.0532 - acc: 0.7693\n",
            "Epoch 44/50000\n",
            " - 2s - loss: 0.0510 - acc: 0.7798\n",
            "Epoch 45/50000\n",
            " - 2s - loss: 0.0473 - acc: 0.7887\n",
            "Epoch 46/50000\n",
            " - 2s - loss: 0.0504 - acc: 0.7723\n",
            "Epoch 47/50000\n",
            " - 2s - loss: 0.0423 - acc: 0.8125\n",
            "Epoch 48/50000\n",
            " - 2s - loss: 0.0471 - acc: 0.7917\n",
            "Epoch 49/50000\n",
            " - 2s - loss: 0.0505 - acc: 0.7708\n",
            "Epoch 50/50000\n",
            " - 2s - loss: 0.0515 - acc: 0.7574\n",
            "Epoch 51/50000\n",
            " - 2s - loss: 0.0419 - acc: 0.7976\n",
            "Epoch 52/50000\n",
            " - 2s - loss: 0.0480 - acc: 0.7798\n",
            "Epoch 53/50000\n",
            " - 2s - loss: 0.0465 - acc: 0.7932\n",
            "Epoch 54/50000\n",
            " - 2s - loss: 0.0431 - acc: 0.8051\n",
            "Epoch 55/50000\n",
            " - 2s - loss: 0.0497 - acc: 0.7723\n",
            "Epoch 56/50000\n",
            " - 2s - loss: 0.0442 - acc: 0.8051\n",
            "Epoch 57/50000\n",
            " - 2s - loss: 0.0474 - acc: 0.7753\n",
            "Epoch 58/50000\n",
            " - 2s - loss: 0.0466 - acc: 0.7798\n",
            "Epoch 59/50000\n",
            " - 2s - loss: 0.0436 - acc: 0.7976\n",
            "Epoch 60/50000\n",
            " - 2s - loss: 0.0441 - acc: 0.8021\n",
            "Epoch 61/50000\n",
            " - 2s - loss: 0.0476 - acc: 0.7723\n",
            "Epoch 62/50000\n",
            " - 2s - loss: 0.0440 - acc: 0.7902\n",
            "Epoch 63/50000\n",
            " - 2s - loss: 0.0449 - acc: 0.7902\n",
            "Epoch 64/50000\n",
            " - 2s - loss: 0.0434 - acc: 0.8080\n",
            "Epoch 65/50000\n",
            " - 2s - loss: 0.0419 - acc: 0.8036\n",
            "Epoch 66/50000\n",
            " - 2s - loss: 0.0424 - acc: 0.8065\n",
            "Epoch 67/50000\n",
            " - 2s - loss: 0.0379 - acc: 0.8244\n",
            "Epoch 68/50000\n",
            " - 2s - loss: 0.0450 - acc: 0.7872\n",
            "Epoch 69/50000\n",
            " - 2s - loss: 0.0457 - acc: 0.7842\n",
            "Epoch 70/50000\n",
            " - 2s - loss: 0.0412 - acc: 0.8021\n",
            "Epoch 71/50000\n",
            " - 2s - loss: 0.0484 - acc: 0.7664\n",
            "Epoch 72/50000\n",
            " - 2s - loss: 0.0432 - acc: 0.7798\n",
            "Epoch 73/50000\n",
            " - 2s - loss: 0.0489 - acc: 0.7574\n",
            "Epoch 74/50000\n",
            " - 2s - loss: 0.0439 - acc: 0.7932\n",
            "Epoch 75/50000\n",
            " - 2s - loss: 0.0389 - acc: 0.8214\n",
            "Epoch 76/50000\n",
            " - 2s - loss: 0.0429 - acc: 0.8036\n",
            "Epoch 77/50000\n",
            " - 2s - loss: 0.0408 - acc: 0.8065\n",
            "Epoch 78/50000\n",
            " - 2s - loss: 0.0374 - acc: 0.8259\n",
            "Epoch 79/50000\n",
            " - 2s - loss: 0.0400 - acc: 0.8051\n",
            "Epoch 80/50000\n",
            " - 2s - loss: 0.0413 - acc: 0.8036\n",
            "Epoch 81/50000\n",
            " - 2s - loss: 0.0440 - acc: 0.7738\n",
            "Epoch 82/50000\n",
            " - 2s - loss: 0.0386 - acc: 0.8155\n",
            "Epoch 83/50000\n",
            " - 2s - loss: 0.0466 - acc: 0.7693\n",
            "Epoch 84/50000\n",
            " - 2s - loss: 0.0425 - acc: 0.7827\n",
            "Epoch 85/50000\n",
            " - 2s - loss: 0.0383 - acc: 0.8274\n",
            "Epoch 86/50000\n",
            " - 2s - loss: 0.0444 - acc: 0.7932\n",
            "Epoch 87/50000\n",
            " - 2s - loss: 0.0425 - acc: 0.7991\n",
            "Epoch 88/50000\n",
            " - 2s - loss: 0.0427 - acc: 0.7917\n",
            "Epoch 89/50000\n",
            " - 2s - loss: 0.0448 - acc: 0.7753\n",
            "Epoch 90/50000\n",
            " - 2s - loss: 0.0417 - acc: 0.7946\n",
            "Epoch 91/50000\n",
            " - 2s - loss: 0.0376 - acc: 0.8110\n",
            "Epoch 92/50000\n",
            " - 2s - loss: 0.0379 - acc: 0.8214\n",
            "Epoch 93/50000\n",
            " - 2s - loss: 0.0394 - acc: 0.8065\n",
            "Epoch 94/50000\n",
            " - 2s - loss: 0.0418 - acc: 0.7932\n",
            "Epoch 95/50000\n",
            " - 2s - loss: 0.0381 - acc: 0.8199\n",
            "Epoch 96/50000\n",
            " - 2s - loss: 0.0432 - acc: 0.7753\n",
            "Epoch 97/50000\n",
            " - 2s - loss: 0.0371 - acc: 0.8095\n",
            "Epoch 98/50000\n",
            " - 2s - loss: 0.0397 - acc: 0.8080\n",
            "Epoch 99/50000\n",
            " - 2s - loss: 0.0388 - acc: 0.8036\n",
            "Epoch 100/50000\n",
            " - 2s - loss: 0.0373 - acc: 0.8185\n",
            "Epoch 101/50000\n",
            " - 2s - loss: 0.0350 - acc: 0.8378\n",
            "Epoch 102/50000\n",
            " - 2s - loss: 0.0383 - acc: 0.8140\n",
            "Epoch 103/50000\n",
            " - 2s - loss: 0.0338 - acc: 0.8393\n",
            "Epoch 104/50000\n",
            " - 2s - loss: 0.0384 - acc: 0.8080\n",
            "Epoch 105/50000\n",
            " - 2s - loss: 0.0418 - acc: 0.7946\n",
            "Epoch 106/50000\n",
            " - 2s - loss: 0.0433 - acc: 0.7902\n",
            "Epoch 107/50000\n",
            " - 2s - loss: 0.0392 - acc: 0.8021\n",
            "Epoch 108/50000\n",
            " - 2s - loss: 0.0356 - acc: 0.8333\n",
            "Epoch 109/50000\n",
            " - 2s - loss: 0.0326 - acc: 0.8571\n",
            "Epoch 110/50000\n",
            " - 2s - loss: 0.0356 - acc: 0.8304\n",
            "Epoch 111/50000\n",
            " - 2s - loss: 0.0365 - acc: 0.8333\n",
            "Epoch 112/50000\n",
            " - 2s - loss: 0.0362 - acc: 0.8318\n",
            "Epoch 113/50000\n",
            " - 2s - loss: 0.0398 - acc: 0.8080\n",
            "Epoch 114/50000\n",
            " - 2s - loss: 0.0391 - acc: 0.8006\n",
            "Epoch 115/50000\n",
            " - 2s - loss: 0.0388 - acc: 0.8170\n",
            "Epoch 116/50000\n",
            " - 2s - loss: 0.0409 - acc: 0.8006\n",
            "Epoch 117/50000\n",
            " - 2s - loss: 0.0405 - acc: 0.8080\n",
            "Epoch 118/50000\n",
            " - 2s - loss: 0.0373 - acc: 0.8185\n",
            "Epoch 119/50000\n",
            " - 2s - loss: 0.0406 - acc: 0.8110\n",
            "Epoch 120/50000\n",
            " - 2s - loss: 0.0384 - acc: 0.8140\n",
            "Epoch 121/50000\n",
            " - 2s - loss: 0.0345 - acc: 0.8318\n",
            "Epoch 122/50000\n",
            " - 2s - loss: 0.0339 - acc: 0.8512\n",
            "Epoch 123/50000\n",
            " - 2s - loss: 0.0400 - acc: 0.8021\n",
            "Epoch 124/50000\n",
            " - 2s - loss: 0.0349 - acc: 0.8437\n",
            "Epoch 125/50000\n",
            " - 2s - loss: 0.0419 - acc: 0.7917\n",
            "Epoch 126/50000\n",
            " - 2s - loss: 0.0368 - acc: 0.8289\n",
            "Epoch 127/50000\n",
            " - 2s - loss: 0.0366 - acc: 0.8229\n",
            "Epoch 128/50000\n",
            " - 2s - loss: 0.0362 - acc: 0.8304\n",
            "Epoch 129/50000\n",
            " - 2s - loss: 0.0345 - acc: 0.8304\n",
            "Epoch 130/50000\n",
            " - 2s - loss: 0.0367 - acc: 0.8378\n",
            "Epoch 131/50000\n",
            " - 2s - loss: 0.0401 - acc: 0.8125\n",
            "Epoch 132/50000\n",
            " - 2s - loss: 0.0362 - acc: 0.8304\n",
            "Epoch 133/50000\n",
            " - 2s - loss: 0.0384 - acc: 0.8214\n",
            "Epoch 134/50000\n",
            " - 2s - loss: 0.0317 - acc: 0.8497\n",
            "Epoch 135/50000\n",
            " - 2s - loss: 0.0356 - acc: 0.8289\n",
            "Epoch 136/50000\n",
            " - 2s - loss: 0.0413 - acc: 0.7946\n",
            "Epoch 137/50000\n",
            " - 2s - loss: 0.0390 - acc: 0.8080\n",
            "Epoch 138/50000\n",
            " - 2s - loss: 0.0313 - acc: 0.8408\n",
            "Epoch 139/50000\n",
            " - 2s - loss: 0.0361 - acc: 0.8274\n",
            "Epoch 140/50000\n",
            " - 2s - loss: 0.0359 - acc: 0.8482\n",
            "Epoch 141/50000\n",
            " - 2s - loss: 0.0371 - acc: 0.8170\n",
            "Epoch 142/50000\n",
            " - 2s - loss: 0.0341 - acc: 0.8363\n",
            "Epoch 143/50000\n",
            " - 2s - loss: 0.0386 - acc: 0.8110\n",
            "Epoch 144/50000\n",
            " - 2s - loss: 0.0353 - acc: 0.8393\n",
            "Epoch 145/50000\n",
            " - 2s - loss: 0.0384 - acc: 0.8140\n",
            "Epoch 146/50000\n",
            " - 2s - loss: 0.0377 - acc: 0.8185\n",
            "Epoch 147/50000\n",
            " - 2s - loss: 0.0355 - acc: 0.8304\n",
            "Epoch 148/50000\n",
            " - 2s - loss: 0.0347 - acc: 0.8274\n",
            "Epoch 149/50000\n",
            " - 2s - loss: 0.0356 - acc: 0.8259\n",
            "Epoch 150/50000\n",
            " - 2s - loss: 0.0380 - acc: 0.8259\n",
            "Epoch 151/50000\n",
            " - 2s - loss: 0.0359 - acc: 0.8229\n",
            "Epoch 152/50000\n",
            " - 2s - loss: 0.0341 - acc: 0.8363\n",
            "Epoch 153/50000\n",
            " - 2s - loss: 0.0374 - acc: 0.8110\n",
            "Epoch 154/50000\n",
            " - 2s - loss: 0.0353 - acc: 0.8244\n",
            "Epoch 155/50000\n",
            " - 2s - loss: 0.0373 - acc: 0.8051\n",
            "Epoch 156/50000\n",
            " - 2s - loss: 0.0346 - acc: 0.8274\n",
            "Epoch 157/50000\n",
            " - 2s - loss: 0.0378 - acc: 0.8170\n",
            "Epoch 158/50000\n",
            " - 2s - loss: 0.0404 - acc: 0.7842\n",
            "Epoch 159/50000\n",
            " - 2s - loss: 0.0366 - acc: 0.8170\n",
            "Epoch 160/50000\n",
            " - 2s - loss: 0.0346 - acc: 0.8318\n",
            "Epoch 161/50000\n",
            " - 2s - loss: 0.0335 - acc: 0.8289\n",
            "Epoch 162/50000\n",
            " - 2s - loss: 0.0371 - acc: 0.8125\n",
            "Epoch 163/50000\n",
            " - 2s - loss: 0.0319 - acc: 0.8482\n",
            "Epoch 164/50000\n",
            " - 2s - loss: 0.0394 - acc: 0.7976\n",
            "Epoch 165/50000\n",
            " - 2s - loss: 0.0314 - acc: 0.8393\n",
            "Epoch 166/50000\n",
            " - 2s - loss: 0.0360 - acc: 0.8259\n",
            "Epoch 167/50000\n",
            " - 2s - loss: 0.0333 - acc: 0.8378\n",
            "Epoch 168/50000\n",
            " - 2s - loss: 0.0306 - acc: 0.8601\n",
            "Epoch 169/50000\n",
            " - 2s - loss: 0.0331 - acc: 0.8497\n",
            "Epoch 170/50000\n",
            " - 2s - loss: 0.0401 - acc: 0.8021\n",
            "Epoch 171/50000\n",
            " - 2s - loss: 0.0339 - acc: 0.8423\n",
            "Epoch 172/50000\n",
            " - 2s - loss: 0.0373 - acc: 0.8125\n",
            "Epoch 173/50000\n",
            " - 2s - loss: 0.0351 - acc: 0.8318\n",
            "Epoch 174/50000\n",
            " - 2s - loss: 0.0351 - acc: 0.8304\n",
            "Epoch 175/50000\n",
            " - 2s - loss: 0.0360 - acc: 0.8318\n",
            "Epoch 176/50000\n",
            " - 2s - loss: 0.0387 - acc: 0.8095\n",
            "Epoch 177/50000\n",
            " - 2s - loss: 0.0329 - acc: 0.8452\n",
            "Epoch 178/50000\n",
            " - 2s - loss: 0.0316 - acc: 0.8497\n",
            "Epoch 179/50000\n",
            " - 2s - loss: 0.0351 - acc: 0.8318\n",
            "Epoch 180/50000\n",
            " - 2s - loss: 0.0369 - acc: 0.8170\n",
            "Epoch 181/50000\n",
            " - 2s - loss: 0.0352 - acc: 0.8244\n",
            "Epoch 182/50000\n",
            " - 2s - loss: 0.0355 - acc: 0.8348\n",
            "Epoch 183/50000\n",
            " - 2s - loss: 0.0329 - acc: 0.8408\n",
            "Epoch 184/50000\n",
            " - 2s - loss: 0.0362 - acc: 0.8155\n",
            "Epoch 185/50000\n",
            " - 2s - loss: 0.0352 - acc: 0.8378\n",
            "Epoch 186/50000\n",
            " - 2s - loss: 0.0382 - acc: 0.8110\n",
            "Epoch 187/50000\n",
            " - 2s - loss: 0.0342 - acc: 0.8467\n",
            "Epoch 188/50000\n",
            " - 2s - loss: 0.0333 - acc: 0.8378\n",
            "Epoch 189/50000\n",
            " - 2s - loss: 0.0362 - acc: 0.8214\n",
            "Epoch 190/50000\n",
            " - 2s - loss: 0.0351 - acc: 0.8185\n",
            "Epoch 191/50000\n",
            " - 2s - loss: 0.0336 - acc: 0.8423\n",
            "Epoch 192/50000\n",
            " - 2s - loss: 0.0356 - acc: 0.8229\n",
            "Epoch 193/50000\n",
            " - 2s - loss: 0.0356 - acc: 0.8199\n",
            "Epoch 194/50000\n",
            " - 2s - loss: 0.0340 - acc: 0.8452\n",
            "Epoch 195/50000\n",
            " - 2s - loss: 0.0344 - acc: 0.8378\n",
            "Epoch 196/50000\n",
            " - 2s - loss: 0.0365 - acc: 0.8140\n",
            "Epoch 197/50000\n",
            " - 2s - loss: 0.0333 - acc: 0.8452\n",
            "Epoch 198/50000\n",
            " - 2s - loss: 0.0348 - acc: 0.8393\n",
            "Epoch 199/50000\n",
            " - 2s - loss: 0.0295 - acc: 0.8646\n",
            "Epoch 200/50000\n",
            " - 2s - loss: 0.0325 - acc: 0.8333\n",
            "Epoch 201/50000\n",
            " - 2s - loss: 0.0397 - acc: 0.8110\n",
            "Epoch 202/50000\n",
            " - 2s - loss: 0.0309 - acc: 0.8631\n",
            "Epoch 203/50000\n",
            " - 2s - loss: 0.0340 - acc: 0.8289\n",
            "Epoch 204/50000\n",
            " - 2s - loss: 0.0329 - acc: 0.8452\n",
            "Epoch 205/50000\n",
            " - 2s - loss: 0.0349 - acc: 0.8378\n",
            "Epoch 206/50000\n",
            " - 2s - loss: 0.0331 - acc: 0.8393\n",
            "Epoch 207/50000\n",
            " - 2s - loss: 0.0344 - acc: 0.8408\n",
            "Epoch 208/50000\n",
            " - 2s - loss: 0.0330 - acc: 0.8408\n",
            "Epoch 209/50000\n",
            " - 2s - loss: 0.0319 - acc: 0.8423\n",
            "Epoch 210/50000\n",
            " - 2s - loss: 0.0349 - acc: 0.8393\n",
            "Epoch 211/50000\n",
            " - 2s - loss: 0.0324 - acc: 0.8408\n",
            "Epoch 212/50000\n",
            " - 2s - loss: 0.0337 - acc: 0.8423\n",
            "Epoch 213/50000\n",
            " - 2s - loss: 0.0366 - acc: 0.8289\n",
            "Epoch 214/50000\n",
            " - 2s - loss: 0.0357 - acc: 0.8244\n",
            "Epoch 215/50000\n",
            " - 2s - loss: 0.0364 - acc: 0.8318\n",
            "Epoch 216/50000\n",
            " - 2s - loss: 0.0377 - acc: 0.8095\n",
            "Epoch 217/50000\n",
            " - 2s - loss: 0.0317 - acc: 0.8393\n",
            "Epoch 218/50000\n",
            " - 2s - loss: 0.0384 - acc: 0.8021\n",
            "Epoch 219/50000\n",
            " - 2s - loss: 0.0341 - acc: 0.8378\n",
            "Epoch 220/50000\n",
            " - 2s - loss: 0.0343 - acc: 0.8348\n",
            "Epoch 221/50000\n",
            " - 2s - loss: 0.0334 - acc: 0.8467\n",
            "Epoch 222/50000\n",
            " - 2s - loss: 0.0338 - acc: 0.8363\n",
            "Epoch 223/50000\n",
            " - 2s - loss: 0.0325 - acc: 0.8467\n",
            "Epoch 224/50000\n",
            " - 2s - loss: 0.0335 - acc: 0.8333\n",
            "Epoch 225/50000\n",
            " - 2s - loss: 0.0371 - acc: 0.8244\n",
            "Epoch 226/50000\n",
            " - 2s - loss: 0.0319 - acc: 0.8408\n",
            "Epoch 227/50000\n",
            " - 2s - loss: 0.0359 - acc: 0.8318\n",
            "Epoch 228/50000\n",
            " - 2s - loss: 0.0362 - acc: 0.8259\n",
            "Epoch 229/50000\n",
            " - 2s - loss: 0.0360 - acc: 0.8259\n",
            "Epoch 230/50000\n",
            " - 2s - loss: 0.0334 - acc: 0.8378\n",
            "Epoch 231/50000\n",
            " - 2s - loss: 0.0345 - acc: 0.8408\n",
            "Epoch 232/50000\n",
            " - 2s - loss: 0.0303 - acc: 0.8646\n",
            "Epoch 233/50000\n",
            " - 2s - loss: 0.0422 - acc: 0.7783\n",
            "Epoch 234/50000\n",
            " - 2s - loss: 0.0330 - acc: 0.8408\n",
            "Epoch 235/50000\n",
            " - 2s - loss: 0.0348 - acc: 0.8333\n",
            "Epoch 236/50000\n",
            " - 2s - loss: 0.0338 - acc: 0.8423\n",
            "Epoch 237/50000\n",
            " - 2s - loss: 0.0362 - acc: 0.8095\n",
            "Epoch 238/50000\n",
            " - 2s - loss: 0.0357 - acc: 0.8363\n",
            "Epoch 239/50000\n",
            " - 2s - loss: 0.0353 - acc: 0.8408\n",
            "Epoch 240/50000\n",
            " - 2s - loss: 0.0346 - acc: 0.8378\n",
            "Epoch 241/50000\n",
            " - 2s - loss: 0.0361 - acc: 0.8259\n",
            "Epoch 242/50000\n",
            " - 2s - loss: 0.0369 - acc: 0.8214\n",
            "Epoch 243/50000\n",
            " - 2s - loss: 0.0339 - acc: 0.8289\n",
            "Epoch 244/50000\n",
            " - 2s - loss: 0.0348 - acc: 0.8438\n",
            "Epoch 245/50000\n",
            " - 2s - loss: 0.0362 - acc: 0.8304\n",
            "Epoch 246/50000\n",
            " - 2s - loss: 0.0355 - acc: 0.8304\n",
            "Epoch 247/50000\n",
            " - 2s - loss: 0.0317 - acc: 0.8482\n",
            "Epoch 248/50000\n",
            " - 2s - loss: 0.0327 - acc: 0.8348\n",
            "Epoch 249/50000\n",
            " - 2s - loss: 0.0302 - acc: 0.8497\n",
            "Epoch 250/50000\n",
            " - 2s - loss: 0.0328 - acc: 0.8378\n",
            "Epoch 251/50000\n",
            " - 2s - loss: 0.0343 - acc: 0.8348\n",
            "Epoch 252/50000\n",
            " - 2s - loss: 0.0336 - acc: 0.8393\n",
            "Epoch 253/50000\n",
            " - 2s - loss: 0.0287 - acc: 0.8676\n",
            "Epoch 254/50000\n",
            " - 2s - loss: 0.0310 - acc: 0.8378\n",
            "Epoch 255/50000\n",
            " - 2s - loss: 0.0322 - acc: 0.8497\n",
            "Epoch 256/50000\n",
            " - 2s - loss: 0.0320 - acc: 0.8304\n",
            "Epoch 257/50000\n",
            " - 2s - loss: 0.0350 - acc: 0.8333\n",
            "Epoch 258/50000\n",
            " - 2s - loss: 0.0324 - acc: 0.8378\n",
            "Epoch 259/50000\n",
            " - 2s - loss: 0.0355 - acc: 0.8348\n",
            "Epoch 260/50000\n",
            " - 2s - loss: 0.0299 - acc: 0.8676\n",
            "Epoch 261/50000\n",
            " - 2s - loss: 0.0299 - acc: 0.8408\n",
            "Epoch 262/50000\n",
            " - 2s - loss: 0.0351 - acc: 0.8170\n",
            "Epoch 263/50000\n",
            " - 2s - loss: 0.0339 - acc: 0.8527\n",
            "Epoch 264/50000\n",
            " - 2s - loss: 0.0350 - acc: 0.8393\n",
            "Epoch 265/50000\n",
            " - 2s - loss: 0.0350 - acc: 0.8244\n",
            "Epoch 266/50000\n",
            " - 2s - loss: 0.0330 - acc: 0.8423\n",
            "Epoch 267/50000\n",
            " - 2s - loss: 0.0315 - acc: 0.8408\n",
            "Epoch 268/50000\n",
            " - 2s - loss: 0.0284 - acc: 0.8497\n",
            "Epoch 269/50000\n",
            " - 2s - loss: 0.0379 - acc: 0.8259\n",
            "Epoch 270/50000\n",
            " - 2s - loss: 0.0336 - acc: 0.8289\n",
            "Epoch 271/50000\n",
            " - 2s - loss: 0.0335 - acc: 0.8363\n",
            "Epoch 272/50000\n",
            " - 2s - loss: 0.0325 - acc: 0.8438\n",
            "Epoch 273/50000\n",
            " - 2s - loss: 0.0284 - acc: 0.8616\n",
            "Epoch 274/50000\n",
            " - 2s - loss: 0.0348 - acc: 0.8289\n",
            "Epoch 275/50000\n",
            " - 2s - loss: 0.0317 - acc: 0.8452\n",
            "Epoch 276/50000\n",
            " - 2s - loss: 0.0343 - acc: 0.8274\n",
            "Epoch 277/50000\n",
            " - 2s - loss: 0.0368 - acc: 0.8289\n",
            "Epoch 278/50000\n",
            " - 2s - loss: 0.0327 - acc: 0.8482\n",
            "Epoch 279/50000\n",
            " - 2s - loss: 0.0369 - acc: 0.8125\n",
            "Epoch 280/50000\n",
            " - 2s - loss: 0.0338 - acc: 0.8274\n",
            "Epoch 281/50000\n",
            " - 2s - loss: 0.0346 - acc: 0.8378\n",
            "Epoch 282/50000\n",
            " - 2s - loss: 0.0295 - acc: 0.8512\n",
            "Epoch 283/50000\n",
            " - 2s - loss: 0.0332 - acc: 0.8318\n",
            "Epoch 284/50000\n",
            " - 2s - loss: 0.0340 - acc: 0.8304\n",
            "Epoch 285/50000\n",
            " - 2s - loss: 0.0327 - acc: 0.8378\n",
            "Epoch 286/50000\n",
            " - 2s - loss: 0.0347 - acc: 0.8170\n",
            "Epoch 287/50000\n",
            " - 2s - loss: 0.0325 - acc: 0.8304\n",
            "Epoch 288/50000\n",
            " - 2s - loss: 0.0302 - acc: 0.8542\n",
            "Epoch 289/50000\n",
            " - 2s - loss: 0.0319 - acc: 0.8527\n",
            "Epoch 290/50000\n",
            " - 2s - loss: 0.0371 - acc: 0.8214\n",
            "Epoch 291/50000\n",
            " - 2s - loss: 0.0355 - acc: 0.8244\n",
            "Epoch 292/50000\n",
            " - 2s - loss: 0.0343 - acc: 0.8274\n",
            "Epoch 293/50000\n",
            " - 2s - loss: 0.0293 - acc: 0.8542\n",
            "Epoch 294/50000\n",
            " - 2s - loss: 0.0338 - acc: 0.8348\n",
            "Epoch 295/50000\n",
            " - 2s - loss: 0.0328 - acc: 0.8378\n",
            "Epoch 296/50000\n",
            " - 2s - loss: 0.0302 - acc: 0.8512\n",
            "Epoch 297/50000\n",
            " - 2s - loss: 0.0353 - acc: 0.8214\n",
            "Epoch 298/50000\n",
            " - 2s - loss: 0.0317 - acc: 0.8482\n",
            "Epoch 299/50000\n",
            " - 2s - loss: 0.0350 - acc: 0.8423\n",
            "Epoch 300/50000\n",
            " - 2s - loss: 0.0335 - acc: 0.8408\n",
            "Epoch 301/50000\n",
            " - 2s - loss: 0.0292 - acc: 0.8646\n",
            "Epoch 302/50000\n",
            " - 2s - loss: 0.0359 - acc: 0.8259\n",
            "Epoch 303/50000\n",
            " - 2s - loss: 0.0351 - acc: 0.8423\n",
            "Epoch 304/50000\n",
            " - 2s - loss: 0.0301 - acc: 0.8661\n",
            "Epoch 305/50000\n",
            " - 2s - loss: 0.0313 - acc: 0.8423\n",
            "Epoch 306/50000\n",
            " - 2s - loss: 0.0324 - acc: 0.8393\n",
            "Epoch 307/50000\n",
            " - 2s - loss: 0.0308 - acc: 0.8527\n",
            "Epoch 308/50000\n",
            " - 2s - loss: 0.0340 - acc: 0.8259\n",
            "Epoch 309/50000\n",
            " - 2s - loss: 0.0348 - acc: 0.8289\n",
            "Epoch 310/50000\n",
            " - 2s - loss: 0.0313 - acc: 0.8393\n",
            "Epoch 311/50000\n",
            " - 2s - loss: 0.0330 - acc: 0.8438\n",
            "Epoch 312/50000\n",
            " - 2s - loss: 0.0336 - acc: 0.8318\n",
            "Epoch 313/50000\n",
            " - 2s - loss: 0.0313 - acc: 0.8527\n",
            "Epoch 314/50000\n",
            " - 2s - loss: 0.0330 - acc: 0.8363\n",
            "Epoch 315/50000\n",
            " - 2s - loss: 0.0320 - acc: 0.8363\n",
            "Epoch 316/50000\n",
            " - 2s - loss: 0.0361 - acc: 0.8185\n",
            "Epoch 317/50000\n",
            " - 2s - loss: 0.0354 - acc: 0.8408\n",
            "Epoch 318/50000\n",
            " - 2s - loss: 0.0360 - acc: 0.8304\n",
            "Epoch 319/50000\n",
            " - 2s - loss: 0.0318 - acc: 0.8467\n",
            "Epoch 320/50000\n",
            " - 2s - loss: 0.0306 - acc: 0.8542\n",
            "Epoch 321/50000\n",
            " - 2s - loss: 0.0351 - acc: 0.8259\n",
            "Epoch 322/50000\n",
            " - 2s - loss: 0.0325 - acc: 0.8408\n",
            "Epoch 323/50000\n",
            " - 2s - loss: 0.0367 - acc: 0.8274\n",
            "Epoch 324/50000\n",
            " - 2s - loss: 0.0332 - acc: 0.8274\n",
            "Epoch 325/50000\n",
            " - 2s - loss: 0.0284 - acc: 0.8601\n",
            "Epoch 326/50000\n",
            " - 2s - loss: 0.0337 - acc: 0.8333\n",
            "Epoch 327/50000\n",
            " - 2s - loss: 0.0312 - acc: 0.8571\n",
            "Epoch 328/50000\n",
            " - 2s - loss: 0.0299 - acc: 0.8571\n",
            "Epoch 329/50000\n",
            " - 2s - loss: 0.0338 - acc: 0.8378\n",
            "Epoch 330/50000\n",
            " - 2s - loss: 0.0310 - acc: 0.8482\n",
            "Epoch 331/50000\n",
            " - 2s - loss: 0.0296 - acc: 0.8527\n",
            "Epoch 332/50000\n",
            " - 2s - loss: 0.0331 - acc: 0.8363\n",
            "Epoch 333/50000\n",
            " - 2s - loss: 0.0345 - acc: 0.8155\n",
            "Epoch 334/50000\n",
            " - 2s - loss: 0.0325 - acc: 0.8378\n",
            "Epoch 335/50000\n",
            " - 2s - loss: 0.0345 - acc: 0.8333\n",
            "Epoch 336/50000\n",
            " - 2s - loss: 0.0301 - acc: 0.8527\n",
            "Epoch 337/50000\n",
            " - 2s - loss: 0.0327 - acc: 0.8408\n",
            "Epoch 338/50000\n",
            " - 2s - loss: 0.0285 - acc: 0.8631\n",
            "Epoch 339/50000\n",
            " - 2s - loss: 0.0355 - acc: 0.8229\n",
            "Epoch 340/50000\n",
            " - 2s - loss: 0.0340 - acc: 0.8289\n",
            "Epoch 341/50000\n",
            " - 2s - loss: 0.0328 - acc: 0.8289\n",
            "Epoch 342/50000\n",
            " - 2s - loss: 0.0307 - acc: 0.8497\n",
            "Epoch 343/50000\n",
            " - 2s - loss: 0.0324 - acc: 0.8393\n",
            "Epoch 344/50000\n",
            " - 2s - loss: 0.0323 - acc: 0.8363\n",
            "Epoch 345/50000\n",
            " - 2s - loss: 0.0345 - acc: 0.8289\n",
            "Epoch 346/50000\n",
            " - 2s - loss: 0.0304 - acc: 0.8497\n",
            "Epoch 347/50000\n",
            " - 2s - loss: 0.0333 - acc: 0.8333\n",
            "Epoch 348/50000\n",
            " - 2s - loss: 0.0310 - acc: 0.8467\n",
            "Epoch 349/50000\n",
            " - 2s - loss: 0.0329 - acc: 0.8259\n",
            "Epoch 350/50000\n",
            " - 2s - loss: 0.0338 - acc: 0.8378\n",
            "Epoch 351/50000\n",
            " - 2s - loss: 0.0304 - acc: 0.8512\n",
            "Epoch 352/50000\n",
            " - 2s - loss: 0.0330 - acc: 0.8423\n",
            "Epoch 353/50000\n",
            " - 2s - loss: 0.0310 - acc: 0.8482\n",
            "Epoch 354/50000\n",
            " - 2s - loss: 0.0340 - acc: 0.8393\n",
            "Epoch 355/50000\n",
            " - 2s - loss: 0.0308 - acc: 0.8482\n",
            "Epoch 356/50000\n",
            " - 2s - loss: 0.0344 - acc: 0.8318\n",
            "Epoch 357/50000\n",
            " - 2s - loss: 0.0319 - acc: 0.8437\n",
            "Epoch 358/50000\n",
            " - 2s - loss: 0.0340 - acc: 0.8333\n",
            "Epoch 359/50000\n",
            " - 2s - loss: 0.0300 - acc: 0.8423\n",
            "Epoch 360/50000\n",
            " - 2s - loss: 0.0364 - acc: 0.8274\n",
            "Epoch 361/50000\n",
            " - 2s - loss: 0.0322 - acc: 0.8482\n",
            "Epoch 362/50000\n",
            " - 2s - loss: 0.0321 - acc: 0.8557\n",
            "Epoch 363/50000\n",
            " - 2s - loss: 0.0340 - acc: 0.8393\n",
            "Epoch 364/50000\n",
            " - 2s - loss: 0.0328 - acc: 0.8452\n",
            "Epoch 365/50000\n",
            " - 2s - loss: 0.0337 - acc: 0.8378\n",
            "Epoch 366/50000\n",
            " - 2s - loss: 0.0293 - acc: 0.8601\n",
            "Epoch 367/50000\n",
            " - 2s - loss: 0.0313 - acc: 0.8512\n",
            "Epoch 368/50000\n",
            " - 2s - loss: 0.0318 - acc: 0.8393\n",
            "Epoch 369/50000\n",
            " - 2s - loss: 0.0273 - acc: 0.8690\n",
            "Epoch 370/50000\n",
            " - 2s - loss: 0.0379 - acc: 0.8185\n",
            "Epoch 371/50000\n",
            " - 2s - loss: 0.0311 - acc: 0.8661\n",
            "Epoch 372/50000\n",
            " - 2s - loss: 0.0313 - acc: 0.8408\n",
            "Epoch 373/50000\n",
            " - 2s - loss: 0.0303 - acc: 0.8423\n",
            "Epoch 374/50000\n",
            " - 2s - loss: 0.0341 - acc: 0.8318\n",
            "Epoch 375/50000\n",
            " - 2s - loss: 0.0336 - acc: 0.8363\n",
            "Epoch 376/50000\n",
            " - 2s - loss: 0.0355 - acc: 0.8378\n",
            "Epoch 377/50000\n",
            " - 2s - loss: 0.0291 - acc: 0.8586\n",
            "Epoch 378/50000\n",
            " - 2s - loss: 0.0336 - acc: 0.8304\n",
            "Epoch 379/50000\n",
            " - 2s - loss: 0.0341 - acc: 0.8244\n",
            "Epoch 380/50000\n",
            " - 2s - loss: 0.0334 - acc: 0.8333\n",
            "Epoch 381/50000\n",
            " - 2s - loss: 0.0318 - acc: 0.8452\n",
            "Epoch 382/50000\n",
            " - 2s - loss: 0.0316 - acc: 0.8333\n",
            "Epoch 383/50000\n",
            " - 2s - loss: 0.0341 - acc: 0.8363\n",
            "Epoch 384/50000\n",
            " - 2s - loss: 0.0296 - acc: 0.8512\n",
            "Epoch 385/50000\n",
            " - 2s - loss: 0.0281 - acc: 0.8542\n",
            "Epoch 386/50000\n",
            " - 2s - loss: 0.0318 - acc: 0.8393\n",
            "Epoch 387/50000\n",
            " - 2s - loss: 0.0342 - acc: 0.8452\n",
            "Epoch 388/50000\n",
            " - 2s - loss: 0.0337 - acc: 0.8259\n",
            "Epoch 389/50000\n",
            " - 2s - loss: 0.0308 - acc: 0.8467\n",
            "Epoch 390/50000\n",
            " - 2s - loss: 0.0346 - acc: 0.8229\n",
            "Epoch 391/50000\n",
            " - 2s - loss: 0.0298 - acc: 0.8631\n",
            "Epoch 392/50000\n",
            " - 2s - loss: 0.0342 - acc: 0.8229\n",
            "Epoch 393/50000\n",
            " - 2s - loss: 0.0329 - acc: 0.8452\n",
            "Epoch 394/50000\n",
            " - 2s - loss: 0.0323 - acc: 0.8393\n",
            "Epoch 395/50000\n",
            " - 2s - loss: 0.0356 - acc: 0.8333\n",
            "Epoch 396/50000\n",
            " - 2s - loss: 0.0332 - acc: 0.8482\n",
            "Epoch 397/50000\n",
            " - 2s - loss: 0.0320 - acc: 0.8333\n",
            "Epoch 398/50000\n",
            " - 2s - loss: 0.0324 - acc: 0.8437\n",
            "Epoch 399/50000\n",
            " - 2s - loss: 0.0310 - acc: 0.8527\n",
            "Epoch 400/50000\n",
            " - 2s - loss: 0.0356 - acc: 0.8274\n",
            "Epoch 401/50000\n",
            " - 2s - loss: 0.0320 - acc: 0.8527\n",
            "Epoch 402/50000\n",
            " - 2s - loss: 0.0282 - acc: 0.8631\n",
            "Epoch 403/50000\n",
            " - 2s - loss: 0.0331 - acc: 0.8438\n",
            "Epoch 404/50000\n",
            " - 2s - loss: 0.0333 - acc: 0.8289\n",
            "Epoch 405/50000\n",
            " - 2s - loss: 0.0359 - acc: 0.8259\n",
            "Epoch 406/50000\n",
            " - 2s - loss: 0.0329 - acc: 0.8289\n",
            "Epoch 407/50000\n",
            " - 2s - loss: 0.0354 - acc: 0.8274\n",
            "Epoch 408/50000\n",
            " - 2s - loss: 0.0319 - acc: 0.8586\n",
            "Epoch 409/50000\n",
            " - 2s - loss: 0.0298 - acc: 0.8661\n",
            "Epoch 410/50000\n",
            " - 2s - loss: 0.0323 - acc: 0.8318\n",
            "Epoch 411/50000\n",
            " - 2s - loss: 0.0341 - acc: 0.8348\n",
            "Epoch 412/50000\n",
            " - 2s - loss: 0.0326 - acc: 0.8378\n",
            "Epoch 413/50000\n",
            " - 2s - loss: 0.0308 - acc: 0.8586\n",
            "Epoch 414/50000\n",
            " - 2s - loss: 0.0350 - acc: 0.8363\n",
            "Epoch 415/50000\n",
            " - 2s - loss: 0.0291 - acc: 0.8631\n",
            "Epoch 416/50000\n",
            " - 2s - loss: 0.0329 - acc: 0.8423\n",
            "Epoch 417/50000\n",
            " - 2s - loss: 0.0308 - acc: 0.8557\n",
            "Epoch 418/50000\n",
            " - 2s - loss: 0.0319 - acc: 0.8527\n",
            "Epoch 419/50000\n",
            " - 2s - loss: 0.0373 - acc: 0.8229\n",
            "Epoch 420/50000\n",
            " - 2s - loss: 0.0303 - acc: 0.8467\n",
            "Epoch 421/50000\n",
            " - 2s - loss: 0.0324 - acc: 0.8542\n",
            "Epoch 422/50000\n",
            " - 2s - loss: 0.0338 - acc: 0.8244\n",
            "Epoch 423/50000\n",
            " - 2s - loss: 0.0313 - acc: 0.8527\n",
            "Epoch 424/50000\n",
            " - 2s - loss: 0.0349 - acc: 0.8244\n",
            "Epoch 425/50000\n",
            " - 2s - loss: 0.0338 - acc: 0.8333\n",
            "Epoch 426/50000\n",
            " - 2s - loss: 0.0322 - acc: 0.8467\n",
            "Epoch 427/50000\n",
            " - 2s - loss: 0.0322 - acc: 0.8363\n",
            "Epoch 428/50000\n",
            " - 2s - loss: 0.0319 - acc: 0.8497\n",
            "Epoch 429/50000\n",
            " - 2s - loss: 0.0287 - acc: 0.8676\n",
            "Epoch 430/50000\n",
            " - 2s - loss: 0.0308 - acc: 0.8482\n",
            "Epoch 431/50000\n",
            " - 2s - loss: 0.0300 - acc: 0.8467\n",
            "Epoch 432/50000\n",
            " - 2s - loss: 0.0328 - acc: 0.8408\n",
            "Epoch 433/50000\n",
            " - 2s - loss: 0.0332 - acc: 0.8527\n",
            "Epoch 434/50000\n",
            " - 2s - loss: 0.0310 - acc: 0.8557\n",
            "Epoch 435/50000\n",
            " - 2s - loss: 0.0345 - acc: 0.8289\n",
            "Epoch 436/50000\n",
            " - 2s - loss: 0.0303 - acc: 0.8482\n",
            "Epoch 437/50000\n",
            " - 2s - loss: 0.0314 - acc: 0.8437\n",
            "Epoch 438/50000\n",
            " - 2s - loss: 0.0339 - acc: 0.8333\n",
            "Epoch 439/50000\n",
            " - 2s - loss: 0.0306 - acc: 0.8438\n",
            "Epoch 440/50000\n",
            " - 2s - loss: 0.0340 - acc: 0.8363\n",
            "Epoch 441/50000\n",
            " - 2s - loss: 0.0322 - acc: 0.8512\n",
            "Epoch 442/50000\n",
            " - 2s - loss: 0.0300 - acc: 0.8557\n",
            "Epoch 443/50000\n",
            " - 2s - loss: 0.0346 - acc: 0.8393\n",
            "Epoch 444/50000\n",
            " - 2s - loss: 0.0322 - acc: 0.8497\n",
            "Epoch 445/50000\n",
            " - 2s - loss: 0.0298 - acc: 0.8497\n",
            "Epoch 446/50000\n",
            " - 2s - loss: 0.0334 - acc: 0.8408\n",
            "Epoch 447/50000\n",
            " - 2s - loss: 0.0311 - acc: 0.8437\n",
            "Epoch 448/50000\n",
            " - 2s - loss: 0.0360 - acc: 0.8125\n",
            "Epoch 449/50000\n",
            " - 2s - loss: 0.0314 - acc: 0.8497\n",
            "Epoch 450/50000\n",
            " - 2s - loss: 0.0333 - acc: 0.8393\n",
            "Epoch 451/50000\n",
            " - 2s - loss: 0.0313 - acc: 0.8452\n",
            "Epoch 452/50000\n",
            " - 2s - loss: 0.0272 - acc: 0.8750\n",
            "Epoch 453/50000\n",
            " - 2s - loss: 0.0295 - acc: 0.8571\n",
            "Epoch 454/50000\n",
            " - 2s - loss: 0.0336 - acc: 0.8348\n",
            "Epoch 455/50000\n",
            " - 2s - loss: 0.0328 - acc: 0.8348\n",
            "Epoch 456/50000\n",
            " - 2s - loss: 0.0336 - acc: 0.8318\n",
            "Epoch 457/50000\n",
            " - 2s - loss: 0.0335 - acc: 0.8408\n",
            "Epoch 458/50000\n",
            " - 2s - loss: 0.0349 - acc: 0.8214\n",
            "Epoch 459/50000\n",
            " - 2s - loss: 0.0303 - acc: 0.8527\n",
            "Epoch 460/50000\n",
            " - 2s - loss: 0.0369 - acc: 0.8244\n",
            "Epoch 461/50000\n",
            " - 2s - loss: 0.0324 - acc: 0.8363\n",
            "Epoch 462/50000\n",
            " - 2s - loss: 0.0309 - acc: 0.8497\n",
            "Epoch 463/50000\n",
            " - 2s - loss: 0.0338 - acc: 0.8199\n",
            "Epoch 464/50000\n",
            " - 2s - loss: 0.0309 - acc: 0.8467\n",
            "Epoch 465/50000\n",
            " - 2s - loss: 0.0317 - acc: 0.8482\n",
            "Epoch 466/50000\n",
            " - 2s - loss: 0.0353 - acc: 0.8155\n",
            "Epoch 467/50000\n",
            " - 2s - loss: 0.0290 - acc: 0.8527\n",
            "Epoch 468/50000\n",
            " - 2s - loss: 0.0287 - acc: 0.8705\n",
            "Epoch 469/50000\n",
            " - 2s - loss: 0.0302 - acc: 0.8408\n",
            "Epoch 470/50000\n",
            " - 2s - loss: 0.0312 - acc: 0.8348\n",
            "Epoch 471/50000\n",
            " - 2s - loss: 0.0327 - acc: 0.8363\n",
            "Epoch 472/50000\n",
            " - 2s - loss: 0.0360 - acc: 0.8304\n",
            "Epoch 473/50000\n",
            " - 2s - loss: 0.0301 - acc: 0.8482\n",
            "Epoch 474/50000\n",
            " - 2s - loss: 0.0359 - acc: 0.8333\n",
            "Epoch 475/50000\n",
            " - 2s - loss: 0.0287 - acc: 0.8586\n",
            "Epoch 476/50000\n",
            " - 2s - loss: 0.0288 - acc: 0.8631\n",
            "Epoch 477/50000\n",
            " - 2s - loss: 0.0330 - acc: 0.8452\n",
            "Epoch 478/50000\n",
            " - 2s - loss: 0.0345 - acc: 0.8289\n",
            "Epoch 479/50000\n",
            " - 2s - loss: 0.0355 - acc: 0.8214\n",
            "Epoch 480/50000\n",
            " - 2s - loss: 0.0282 - acc: 0.8690\n",
            "Epoch 481/50000\n",
            " - 2s - loss: 0.0328 - acc: 0.8199\n",
            "Epoch 482/50000\n",
            " - 2s - loss: 0.0346 - acc: 0.8244\n",
            "Epoch 483/50000\n",
            " - 2s - loss: 0.0297 - acc: 0.8497\n",
            "Epoch 484/50000\n",
            " - 2s - loss: 0.0324 - acc: 0.8482\n",
            "Epoch 485/50000\n",
            " - 2s - loss: 0.0295 - acc: 0.8631\n",
            "Epoch 486/50000\n",
            " - 2s - loss: 0.0282 - acc: 0.8631\n",
            "Epoch 487/50000\n",
            " - 2s - loss: 0.0335 - acc: 0.8408\n",
            "Epoch 488/50000\n",
            " - 2s - loss: 0.0368 - acc: 0.8259\n",
            "Epoch 489/50000\n",
            " - 2s - loss: 0.0324 - acc: 0.8497\n",
            "Epoch 490/50000\n",
            " - 2s - loss: 0.0329 - acc: 0.8452\n",
            "Epoch 491/50000\n",
            " - 2s - loss: 0.0336 - acc: 0.8318\n",
            "Epoch 492/50000\n",
            " - 2s - loss: 0.0327 - acc: 0.8318\n",
            "Epoch 493/50000\n",
            " - 2s - loss: 0.0286 - acc: 0.8646\n",
            "Epoch 494/50000\n",
            " - 2s - loss: 0.0292 - acc: 0.8616\n",
            "Epoch 495/50000\n",
            " - 2s - loss: 0.0306 - acc: 0.8497\n",
            "Epoch 496/50000\n",
            " - 2s - loss: 0.0392 - acc: 0.8155\n",
            "Epoch 497/50000\n",
            " - 2s - loss: 0.0333 - acc: 0.8393\n",
            "Epoch 498/50000\n",
            " - 2s - loss: 0.0327 - acc: 0.8393\n",
            "Epoch 499/50000\n",
            " - 2s - loss: 0.0313 - acc: 0.8586\n",
            "Epoch 500/50000\n",
            " - 2s - loss: 0.0310 - acc: 0.8438\n",
            "Epoch 501/50000\n",
            " - 2s - loss: 0.0333 - acc: 0.8408\n",
            "Epoch 502/50000\n",
            " - 2s - loss: 0.0297 - acc: 0.8705\n",
            "Epoch 503/50000\n",
            " - 2s - loss: 0.0301 - acc: 0.8557\n",
            "Epoch 504/50000\n",
            " - 2s - loss: 0.0322 - acc: 0.8363\n",
            "Epoch 505/50000\n",
            " - 2s - loss: 0.0269 - acc: 0.8690\n",
            "Epoch 506/50000\n",
            " - 2s - loss: 0.0331 - acc: 0.8304\n",
            "Epoch 507/50000\n",
            " - 2s - loss: 0.0331 - acc: 0.8467\n",
            "Epoch 508/50000\n",
            " - 2s - loss: 0.0339 - acc: 0.8289\n",
            "Epoch 509/50000\n",
            " - 2s - loss: 0.0294 - acc: 0.8690\n",
            "Epoch 510/50000\n",
            " - 2s - loss: 0.0279 - acc: 0.8616\n",
            "Epoch 511/50000\n",
            " - 2s - loss: 0.0352 - acc: 0.8140\n",
            "Epoch 512/50000\n",
            " - 2s - loss: 0.0311 - acc: 0.8408\n",
            "Epoch 513/50000\n",
            " - 2s - loss: 0.0326 - acc: 0.8348\n",
            "Epoch 514/50000\n",
            " - 2s - loss: 0.0292 - acc: 0.8616\n",
            "Epoch 515/50000\n",
            " - 2s - loss: 0.0326 - acc: 0.8408\n",
            "Epoch 516/50000\n",
            " - 2s - loss: 0.0326 - acc: 0.8378\n",
            "Epoch 517/50000\n",
            " - 2s - loss: 0.0324 - acc: 0.8304\n",
            "Epoch 518/50000\n",
            " - 2s - loss: 0.0315 - acc: 0.8512\n",
            "Epoch 519/50000\n",
            " - 2s - loss: 0.0303 - acc: 0.8467\n",
            "Epoch 520/50000\n",
            " - 2s - loss: 0.0324 - acc: 0.8408\n",
            "Epoch 521/50000\n",
            " - 2s - loss: 0.0295 - acc: 0.8690\n",
            "Epoch 522/50000\n",
            " - 2s - loss: 0.0317 - acc: 0.8378\n",
            "Epoch 523/50000\n",
            " - 2s - loss: 0.0300 - acc: 0.8705\n",
            "Epoch 524/50000\n",
            " - 2s - loss: 0.0324 - acc: 0.8452\n",
            "Epoch 525/50000\n",
            " - 2s - loss: 0.0315 - acc: 0.8408\n",
            "Epoch 526/50000\n",
            " - 2s - loss: 0.0301 - acc: 0.8423\n",
            "Epoch 527/50000\n",
            " - 2s - loss: 0.0334 - acc: 0.8348\n",
            "Epoch 528/50000\n",
            " - 2s - loss: 0.0332 - acc: 0.8452\n",
            "Epoch 529/50000\n",
            " - 2s - loss: 0.0334 - acc: 0.8437\n",
            "Epoch 530/50000\n",
            " - 2s - loss: 0.0329 - acc: 0.8333\n",
            "Epoch 531/50000\n",
            " - 2s - loss: 0.0308 - acc: 0.8437\n",
            "Epoch 532/50000\n",
            " - 2s - loss: 0.0316 - acc: 0.8482\n",
            "Epoch 533/50000\n",
            " - 2s - loss: 0.0319 - acc: 0.8408\n",
            "Epoch 534/50000\n",
            " - 2s - loss: 0.0325 - acc: 0.8423\n",
            "Epoch 535/50000\n",
            " - 2s - loss: 0.0313 - acc: 0.8512\n",
            "Epoch 536/50000\n",
            " - 2s - loss: 0.0367 - acc: 0.8155\n",
            "Epoch 537/50000\n",
            " - 2s - loss: 0.0332 - acc: 0.8512\n",
            "Epoch 538/50000\n",
            " - 2s - loss: 0.0347 - acc: 0.8318\n",
            "Epoch 539/50000\n",
            " - 2s - loss: 0.0355 - acc: 0.8378\n",
            "Epoch 540/50000\n",
            " - 2s - loss: 0.0326 - acc: 0.8393\n",
            "Epoch 541/50000\n",
            " - 2s - loss: 0.0325 - acc: 0.8423\n",
            "Epoch 542/50000\n",
            " - 2s - loss: 0.0262 - acc: 0.8705\n",
            "Epoch 543/50000\n",
            " - 2s - loss: 0.0326 - acc: 0.8304\n",
            "Epoch 544/50000\n",
            " - 2s - loss: 0.0329 - acc: 0.8378\n",
            "Epoch 545/50000\n",
            " - 2s - loss: 0.0326 - acc: 0.8452\n",
            "Epoch 546/50000\n",
            " - 2s - loss: 0.0305 - acc: 0.8542\n",
            "Epoch 547/50000\n",
            " - 2s - loss: 0.0307 - acc: 0.8438\n",
            "Epoch 548/50000\n",
            " - 2s - loss: 0.0298 - acc: 0.8586\n",
            "Epoch 549/50000\n",
            " - 2s - loss: 0.0329 - acc: 0.8482\n",
            "Epoch 550/50000\n",
            " - 2s - loss: 0.0333 - acc: 0.8378\n",
            "Epoch 551/50000\n",
            " - 2s - loss: 0.0324 - acc: 0.8423\n",
            "Epoch 552/50000\n",
            " - 2s - loss: 0.0311 - acc: 0.8423\n",
            "Epoch 553/50000\n",
            " - 2s - loss: 0.0325 - acc: 0.8393\n",
            "Epoch 554/50000\n",
            " - 2s - loss: 0.0327 - acc: 0.8333\n",
            "Epoch 555/50000\n",
            " - 2s - loss: 0.0270 - acc: 0.8676\n",
            "Epoch 556/50000\n",
            " - 2s - loss: 0.0304 - acc: 0.8467\n",
            "Epoch 557/50000\n",
            " - 2s - loss: 0.0304 - acc: 0.8527\n",
            "Epoch 558/50000\n",
            " - 2s - loss: 0.0315 - acc: 0.8423\n",
            "Epoch 559/50000\n",
            " - 2s - loss: 0.0307 - acc: 0.8497\n",
            "Epoch 560/50000\n",
            " - 2s - loss: 0.0357 - acc: 0.8318\n",
            "Epoch 561/50000\n",
            " - 2s - loss: 0.0341 - acc: 0.8333\n",
            "Epoch 562/50000\n",
            " - 2s - loss: 0.0329 - acc: 0.8467\n",
            "Epoch 563/50000\n",
            " - 2s - loss: 0.0340 - acc: 0.8378\n",
            "Epoch 564/50000\n",
            " - 2s - loss: 0.0330 - acc: 0.8333\n",
            "Epoch 565/50000\n",
            " - 2s - loss: 0.0288 - acc: 0.8586\n",
            "Epoch 566/50000\n",
            " - 2s - loss: 0.0315 - acc: 0.8527\n",
            "Epoch 567/50000\n",
            " - 2s - loss: 0.0331 - acc: 0.8304\n",
            "Epoch 568/50000\n",
            " - 2s - loss: 0.0342 - acc: 0.8378\n",
            "Epoch 569/50000\n",
            " - 2s - loss: 0.0330 - acc: 0.8467\n",
            "Epoch 570/50000\n",
            " - 2s - loss: 0.0288 - acc: 0.8542\n",
            "Epoch 571/50000\n",
            " - 2s - loss: 0.0323 - acc: 0.8378\n",
            "Epoch 572/50000\n",
            " - 2s - loss: 0.0313 - acc: 0.8542\n",
            "Epoch 573/50000\n",
            " - 2s - loss: 0.0336 - acc: 0.8289\n",
            "Epoch 574/50000\n",
            " - 2s - loss: 0.0299 - acc: 0.8616\n",
            "Epoch 575/50000\n",
            " - 2s - loss: 0.0314 - acc: 0.8393\n",
            "Epoch 576/50000\n",
            " - 2s - loss: 0.0323 - acc: 0.8497\n",
            "Epoch 577/50000\n",
            " - 2s - loss: 0.0298 - acc: 0.8497\n",
            "Epoch 578/50000\n",
            " - 2s - loss: 0.0277 - acc: 0.8690\n",
            "Epoch 579/50000\n",
            " - 2s - loss: 0.0283 - acc: 0.8586\n",
            "Epoch 580/50000\n",
            " - 2s - loss: 0.0351 - acc: 0.8304\n",
            "Epoch 581/50000\n",
            " - 2s - loss: 0.0319 - acc: 0.8408\n",
            "Epoch 582/50000\n",
            " - 2s - loss: 0.0295 - acc: 0.8467\n",
            "Epoch 583/50000\n",
            " - 2s - loss: 0.0265 - acc: 0.8705\n",
            "Epoch 584/50000\n",
            " - 2s - loss: 0.0347 - acc: 0.8333\n",
            "Epoch 585/50000\n",
            " - 2s - loss: 0.0305 - acc: 0.8631\n",
            "Epoch 586/50000\n",
            " - 2s - loss: 0.0304 - acc: 0.8571\n",
            "Epoch 587/50000\n",
            " - 2s - loss: 0.0344 - acc: 0.8333\n",
            "Epoch 588/50000\n",
            " - 2s - loss: 0.0299 - acc: 0.8482\n",
            "Epoch 589/50000\n",
            " - 2s - loss: 0.0280 - acc: 0.8631\n",
            "Epoch 590/50000\n",
            " - 2s - loss: 0.0295 - acc: 0.8571\n",
            "Epoch 591/50000\n",
            " - 2s - loss: 0.0336 - acc: 0.8274\n",
            "Epoch 592/50000\n",
            " - 2s - loss: 0.0359 - acc: 0.8185\n",
            "Epoch 593/50000\n",
            " - 2s - loss: 0.0305 - acc: 0.8497\n",
            "Epoch 594/50000\n",
            " - 2s - loss: 0.0320 - acc: 0.8363\n",
            "Epoch 595/50000\n",
            " - 2s - loss: 0.0309 - acc: 0.8408\n",
            "Epoch 596/50000\n",
            " - 2s - loss: 0.0277 - acc: 0.8676\n",
            "Epoch 597/50000\n",
            " - 2s - loss: 0.0302 - acc: 0.8437\n",
            "Epoch 598/50000\n",
            " - 2s - loss: 0.0344 - acc: 0.8229\n",
            "Epoch 599/50000\n",
            " - 2s - loss: 0.0316 - acc: 0.8452\n",
            "Epoch 600/50000\n",
            " - 2s - loss: 0.0329 - acc: 0.8408\n",
            "Epoch 601/50000\n",
            " - 2s - loss: 0.0271 - acc: 0.8631\n",
            "Epoch 602/50000\n",
            " - 2s - loss: 0.0329 - acc: 0.8423\n",
            "Epoch 603/50000\n",
            " - 2s - loss: 0.0354 - acc: 0.8318\n",
            "Epoch 604/50000\n",
            " - 2s - loss: 0.0335 - acc: 0.8274\n",
            "Epoch 605/50000\n",
            " - 2s - loss: 0.0303 - acc: 0.8571\n",
            "Epoch 606/50000\n",
            " - 2s - loss: 0.0287 - acc: 0.8571\n",
            "Epoch 607/50000\n",
            " - 2s - loss: 0.0330 - acc: 0.8437\n",
            "Epoch 608/50000\n",
            " - 2s - loss: 0.0307 - acc: 0.8452\n",
            "Epoch 609/50000\n",
            " - 2s - loss: 0.0312 - acc: 0.8452\n",
            "Epoch 610/50000\n",
            " - 2s - loss: 0.0325 - acc: 0.8423\n",
            "Epoch 611/50000\n",
            " - 2s - loss: 0.0310 - acc: 0.8408\n",
            "Epoch 612/50000\n",
            " - 2s - loss: 0.0311 - acc: 0.8512\n",
            "Epoch 613/50000\n",
            " - 2s - loss: 0.0318 - acc: 0.8438\n",
            "Epoch 614/50000\n",
            " - 2s - loss: 0.0316 - acc: 0.8438\n",
            "Epoch 615/50000\n",
            " - 2s - loss: 0.0332 - acc: 0.8438\n",
            "Epoch 616/50000\n",
            " - 2s - loss: 0.0318 - acc: 0.8363\n",
            "Epoch 617/50000\n",
            " - 2s - loss: 0.0299 - acc: 0.8557\n",
            "Epoch 618/50000\n",
            " - 2s - loss: 0.0304 - acc: 0.8423\n",
            "Epoch 619/50000\n",
            " - 2s - loss: 0.0306 - acc: 0.8393\n",
            "Epoch 620/50000\n",
            " - 2s - loss: 0.0340 - acc: 0.8244\n",
            "Epoch 621/50000\n",
            " - 2s - loss: 0.0296 - acc: 0.8601\n",
            "Epoch 622/50000\n",
            " - 2s - loss: 0.0339 - acc: 0.8318\n",
            "Epoch 623/50000\n",
            " - 2s - loss: 0.0359 - acc: 0.8259\n",
            "Epoch 624/50000\n",
            " - 2s - loss: 0.0334 - acc: 0.8363\n",
            "Epoch 625/50000\n",
            " - 2s - loss: 0.0302 - acc: 0.8542\n",
            "Epoch 626/50000\n",
            " - 2s - loss: 0.0272 - acc: 0.8601\n",
            "Epoch 627/50000\n",
            " - 2s - loss: 0.0365 - acc: 0.8155\n",
            "Epoch 628/50000\n",
            " - 2s - loss: 0.0329 - acc: 0.8348\n",
            "Epoch 629/50000\n",
            " - 2s - loss: 0.0278 - acc: 0.8661\n",
            "Epoch 630/50000\n",
            " - 2s - loss: 0.0290 - acc: 0.8601\n",
            "Epoch 631/50000\n",
            " - 2s - loss: 0.0312 - acc: 0.8452\n",
            "Epoch 632/50000\n",
            " - 2s - loss: 0.0261 - acc: 0.8765\n",
            "Epoch 633/50000\n",
            " - 2s - loss: 0.0293 - acc: 0.8557\n",
            "Epoch 634/50000\n",
            " - 2s - loss: 0.0295 - acc: 0.8571\n",
            "Epoch 635/50000\n",
            " - 2s - loss: 0.0301 - acc: 0.8467\n",
            "Epoch 636/50000\n",
            " - 2s - loss: 0.0319 - acc: 0.8423\n",
            "Epoch 637/50000\n",
            " - 2s - loss: 0.0352 - acc: 0.8318\n",
            "Epoch 638/50000\n",
            " - 2s - loss: 0.0295 - acc: 0.8571\n",
            "Epoch 639/50000\n",
            " - 2s - loss: 0.0293 - acc: 0.8631\n",
            "Epoch 640/50000\n",
            " - 2s - loss: 0.0314 - acc: 0.8408\n",
            "Epoch 641/50000\n",
            " - 2s - loss: 0.0305 - acc: 0.8527\n",
            "Epoch 642/50000\n",
            " - 2s - loss: 0.0349 - acc: 0.8304\n",
            "Epoch 643/50000\n",
            " - 2s - loss: 0.0303 - acc: 0.8571\n",
            "Epoch 644/50000\n",
            " - 2s - loss: 0.0265 - acc: 0.8705\n",
            "Epoch 645/50000\n",
            " - 2s - loss: 0.0312 - acc: 0.8348\n",
            "Epoch 646/50000\n",
            " - 2s - loss: 0.0291 - acc: 0.8631\n",
            "Epoch 647/50000\n",
            " - 2s - loss: 0.0319 - acc: 0.8333\n",
            "Epoch 648/50000\n",
            " - 2s - loss: 0.0304 - acc: 0.8467\n",
            "Epoch 649/50000\n",
            " - 2s - loss: 0.0281 - acc: 0.8646\n",
            "Epoch 650/50000\n",
            " - 2s - loss: 0.0318 - acc: 0.8497\n",
            "Epoch 651/50000\n",
            " - 2s - loss: 0.0324 - acc: 0.8318\n",
            "Epoch 652/50000\n",
            " - 2s - loss: 0.0288 - acc: 0.8646\n",
            "Epoch 653/50000\n",
            " - 2s - loss: 0.0312 - acc: 0.8542\n",
            "Epoch 654/50000\n",
            " - 2s - loss: 0.0320 - acc: 0.8378\n",
            "Epoch 655/50000\n",
            " - 2s - loss: 0.0291 - acc: 0.8497\n",
            "Epoch 656/50000\n",
            " - 2s - loss: 0.0321 - acc: 0.8423\n",
            "Epoch 657/50000\n",
            " - 2s - loss: 0.0325 - acc: 0.8333\n",
            "Epoch 658/50000\n",
            " - 2s - loss: 0.0312 - acc: 0.8452\n",
            "Epoch 659/50000\n",
            " - 2s - loss: 0.0334 - acc: 0.8363\n",
            "Epoch 660/50000\n",
            " - 2s - loss: 0.0373 - acc: 0.8244\n",
            "Epoch 661/50000\n",
            " - 2s - loss: 0.0289 - acc: 0.8661\n",
            "Epoch 662/50000\n",
            " - 2s - loss: 0.0292 - acc: 0.8765\n",
            "Epoch 663/50000\n",
            " - 2s - loss: 0.0324 - acc: 0.8333\n",
            "Epoch 664/50000\n",
            " - 2s - loss: 0.0334 - acc: 0.8378\n",
            "Epoch 665/50000\n",
            " - 2s - loss: 0.0308 - acc: 0.8467\n",
            "Epoch 666/50000\n",
            " - 2s - loss: 0.0257 - acc: 0.8735\n",
            "Epoch 667/50000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-5bb13374f39f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                         \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m                   \u001b[0;31m#訓練週期 此例設定為20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m                \u001b[0;31m#訓練批次筆數 此例設定為2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                         \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m                    \u001b[0;31m#顯示訓練過程\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                        )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Oy0diUkU8Ia"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('Recognize_Ver4.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}